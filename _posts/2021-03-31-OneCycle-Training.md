---
keywords: fastai
description: how to find the best learning rate and add OneCycle training to your deep learning model
title: Handling the Woes of Training
toc: true 
badges: true
comments: true
author: Aditya Rana
image: images/cycle_crying_kid.jpg
categories: [PyTorch]
nb_path: _notebooks/2021-03-31-OneCycle-Training.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-03-31-OneCycle-Training.ipynb
-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Who-needs-to-read-this-post?">Who needs to read this post?<a class="anchor-link" href="#Who-needs-to-read-this-post?"> </a></h2><p>This post is useful if you're training deep learning models (eg. ResNets) for tasks like Image Classification, Object Detection, Image Segmentation and many more. You can use the approaches mentioned here for both training from scratch as well as for fine-tuning. This post is meant to answer questions like</p>
<ul>
<li>what is the best learning rate for my model?<blockquote><p><span style='color:Green'> we'll look at a learning rate finding poicy that takes less than a minute to run</span></p>
</blockquote>
</li>
<li>how to train my models much faster and spend less compute time?<blockquote><p><span style='color:Green'> the models train faster as this approach allows us to use much higher learning rates for training that would otherwise be unsuitable </span></p>
</blockquote>
</li>
<li>how does it benefit my model?<blockquote><p><span style='color:Green'>using higher learning rates help us avoid getting stuck in local minimas</span></p>
</blockquote>
</li>
<li>how can I schedule my learning rate to get the best performance?<blockquote><p><span style='color:Green'> we will implement the OneCycle training policy that this post is about</span></p>
</blockquote>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="TL;DR-Show-me-the-code">TL;DR Show me the code<a class="anchor-link" href="#TL;DR-Show-me-the-code"> </a></h2><p>These ideas were popularized by the <a href="https://docs.fast.ai/">fastai</a> library which is based on PyTorch, but implements them using Callbacks in their custom training script. If you just want to quickly test and add LRFinder and OneCycle learning rate schedule to your training pipeline, you can directly adapt the code below to your script.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="LR-Range-Test">LR Range Test<a class="anchor-link" href="#LR-Range-Test"> </a></h3><p>If you're like me, you would just put 3e-4 into an Adam optimizer and let the model train. But in the last few years, a lot has happened that has made it easier to find the optimal learning rate for our model</p>
<p>In 2015, Leslie N. Smith came up with a trial-and-error technique called the LR Range Test. The idea is simple, you just run your model and data for a few iterations, with the learning rate initially starting at a very small value and then increasing linearly/exponentially after each iteration. We assume that the optimal learning rate is bound to lie between these two extremas, usually taken as [1e-7, 10]. You record the loss for each value of learning rate and plot it up. The low initial learning rate allows the network to start converging and as the learning rate is increased it will eventually be too large and the network will diverge.</p>
<p>A plot for LR Range test should consist of all 3 regions, the first is where the learning rate is too small that loss barely decreases, the “just right” region where loss converges quickly, and the last region where learning rate is too big that loss starts to diverge.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html max-width="400" file="/blog/images/copied_from_nb/images/lr_range_test.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For this test, you can use the library <a href="https://github.com/davidtvs/pytorch-lr-finder">pytorch-lr-finder</a> for finding the best learning rate for your PyTorch model. If you are using PyTorch Lighting, you can use their builtin <a href="https://pytorch-lightning.readthedocs.io/en/latest/advanced/lr_finder.html">lr_finder </a> module. A keras implementation is also available <a href="https://github.com/surmenok/keras_lr_finder">here</a>. As you will see later in the post, implementing this finder is pretty straightforward once you understand the method, but I'm linking these libraries here only to give you a headstart.</p>
<p>This is the plot we want to obtain and analyze</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html max-width="400" file="/blog/images/copied_from_nb/images/lr_finder_lightning.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>How to Interpret this</strong></p>
<p>It is recommended to not pick the learning rate that achieves the lowest loss, but instead something in the middle of the sharpest downward slope (red point), as this is the point at which loss is still decreasing, whereas at the lowest point, the loss has already started increasing.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="OneCycle-Training">OneCycle Training<a class="anchor-link" href="#OneCycle-Training"> </a></h3><p>The <a href="https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.OneCycleLR">OneCyle scheduler</a> is directly available for use in PyTorch. Some things to keep in mind:</p>
<ol>
<li><p>You have to call <code>scheduler.step()</code> after every batch. This is unlike most schedulers which are called after every epoch.</p>
</li>
<li><p>OneCycle works only with optimizers that use momentum (they track the running average of gradients) like SGD, Adam and RMSProp but it won't work with AdaDelta or Adagrad which only track the running average of squared gradients. You'll understand why when we go into details.</p>
</li>
<li><p>In my experience, Adam optimizer has worked the best with this schedule.</p>
</li>
</ol>
<p>The idea is to decrease the momentum when increasing the learning rate and to increase it when decreasing the learning rate. With this policy, the author demonstrates an event called “super-convergence”, where it reaches the same validation accuracy in only 1/5 of the iterations.</p>
<p>{% include image.html max-width="700" file="/blog/images/copied_from_nb/images/lr_mom_iter.png" %}</p>
<p>{% include image.html file="/blog/images/copied_from_nb/images/onecycle_results.png" %}</p>
<p>Let's get down to code</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_lr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;&quot;</span>
<span class="sd">    for tracking how your learning rate is changing throughout training</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">fit_one_cycle</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">max_lr</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">):</span>
    <span class="n">history</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Set up optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_lr</span><span class="p">)</span>
    <span class="c1"># Set up one-cycle learning rate scheduler </span>
    <span class="n">sched</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">OneCycleLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">max_lr</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">=</span> <span class="n">epochs</span><span class="p">,</span> 
                                                <span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="c1"># Training Phase </span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="c1"># Record loss</span>
            <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

            <span class="c1"># Record &amp; update learning rate</span>
            <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">get_lr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">))</span>
            <span class="n">sched</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># Validation phase</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;train_loss&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">train_losses</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;lrs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lrs</span>
        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">history</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you want to see the entire training script in action, you can follow the notebooks presented below</p>
<ol>
<li><a href="https://github.com/adityassrana/object-detection-mnist">Object Detection MNIST</a></li>
<li><a href="https://jovian.ai/aakashns/05b-cifar10-resnet">Image Classification - CIFAR10</a></li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="TBD">TBD<a class="anchor-link" href="#TBD"> </a></h2><p>This post is a work-in-progress and will be completed in the coming days with the theoretical analysis of these ideas.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Papers-Discussed">Papers Discussed<a class="anchor-link" href="#Papers-Discussed"> </a></h2><ol>
<li><a href="https://arxiv.org/abs/1506.01186">Cyclical Learning Rates for Training Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1803.09820">A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay</a></li>
<li><a href="https://arxiv.org/abs/1708.07120">Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates</a></li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Extra-Readings">Extra Readings<a class="anchor-link" href="#Extra-Readings"> </a></h2><p>If you want to get deep (pun intended) into playing around with these concepts I would highly recommend you to watch fast.ai's <a href="https://course19.fast.ai/part2">Deep Learning from the Foundations</a>, spending most of your time reimplementing the notebooks by yourself. Here are some other resources that talk about these ideas</p>
<ol>
<li><a href="https://sgugger.github.io/the-1cycle-policy.html">https://sgugger.github.io/the-1cycle-policy.html</a></li>
<li><a href="https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html#how-do-you-find-a-good-learning-rate">https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html#how-do-you-find-a-good-learning-rate</a></li>
<li><a href="https://medium.com/dsnet/the-1-cycle-policy-an-experiment-that-vanished-the-struggle-in-training-neural-nets-184417de23b9">https://medium.com/dsnet/the-1-cycle-policy-an-experiment-that-vanished-the-struggle-in-training-neural-nets-184417de23b9</a></li>
<li><a href="https://medium.com/vitalify-asia/whats-up-with-deep-learning-optimizers-since-adam-5c1d862b9db0">https://medium.com/vitalify-asia/whats-up-with-deep-learning-optimizers-since-adam-5c1d862b9db0</a></li>
</ol>

</div>
</div>
</div>
</div>
 

