{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Modern PyTorch\n",
    "> my thoughts on some popular PyTorch libraries and good coding practices\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Aditya Rana\n",
    "- image: images/memecaptain.jpeg\n",
    "- categories: [programming, PyTorch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "<img src = \"images/pytorch-logo-dark.png\" height=\"400\" width=\"400\">\n",
    "\n",
    "Learning a programming language/framework is a lot like learning foreign languages. Just studying them online or from a book is hardly sufficient and one needs to get actively involved in conversations and discussions to get the pronunciation and flow of speaking right. The same goes for writing code, so get started on the [PyTorch Forums](https://discuss.pytorch.org/) and [Stack Overflow](https://stackoverflow.com/search?q=pytorch).\n",
    "\n",
    "I'm writing this post after 2 years of using PyTorch, after having started learning from Udacity courses and online blogs, to  heavily experimenting with PyTorch's functionalities during my bachelor's thesis, and then more recently having finished the [fast.ai's Deep Learning from Foundations](https://course19.fast.ai/part2) where Jeremy Howard recreates several core modules of PyTorch and discusses his thought process on creating the latest fastai library.  \n",
    "\n",
    "The purpose of this post is to not to be an all-purpose tutorial or template since there're already a lot of amazing people out there teaching PyTorch, but instead it aims to answer the FAQs and guide people to the right resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "<img src = \"images/fastai.png\" height=\"400\" width=\"400\">\n",
    "\n",
    "\n",
    "If I had to start learning PyTorch all over again, I wouldn't think twice and dig deep into everything Jeremy Howard has to offer. He is the co-founder of [fast.ai](https://www.fast.ai/) along with Rachel Thomas and every year they release several courses on deep learning for **free**. This is world-class educational content you can enjoy at no cost, not even any ads or sponsors \\* gasps \\*. There aren't many quality things in the world that come for free so I would definitely recommend you to check [fast.ai](https://www.fast.ai/) out.\n",
    "\n",
    "If you're confused where to start from all the courses offered, I would suggest watching the first 2-3 videos of their most recent offering of \"Practical Deep Learning for Coders\", and in parallel starting with the course [Deep Learning from Foundations](https://course19.fast.ai/part2)\n",
    "\n",
    "If you're looking for a fast-track introduction to PyTorch, you can read this tutorial [\"What is torch.nn really?\"](https://pytorch.org/tutorials/beginner/nn_tutorial.html) by Jeremy on the official PyTorch page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Educational vs Practical\n",
    "\n",
    "At this point, I must also point out that I have sort of a love-hate relationship with fastai owing to some aspects of their coding style. Even though I've been bluntly advertising them since the start of this post, I do not use the fastai library so often for my projects. I think fastai is the best educational library out there which will get you SOTA results for tasks like Image Classification, Segmentation and a bunch of other tasks in less than 10 line of code, but the fact that instead of developing their library around existing PyTorch functionalities and supporting them, they have tried to create their own abstractions by rewriting PyTorch modules like DataLoaders and introduced their own optimizers, without providing enough extra utility to balance the trade-off.\n",
    "\n",
    "What I do use regularly from fastai is a tonne of ideas that I learned while doing their courses like proper weight initialization, learning rate finder, OneCycle training policy, callbacks, PyTorch Hooks and visualizing layer histograms, just to name a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch vs Keras/TF\n",
    "\n",
    "Sometimes you would see people fighting over PyTorch vs Keras/Tensorflow and which is better. I am not the person who enjoys such debates, or even support PyTorch for that matter. I believe there's a variety of people out there who have their own programming style preferences, and whichever framework suits their taste better, they should use it. I feel more comfortable and cognitively at ease using PyTorch and that's why I prefer the PyTorch ecosystem, but at the same time I don't mind working with Tensorflow whenever I have to.\n",
    "\n",
    "I do like PyTorch vs TF memes though, who doesn't?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/meme1.png\" height=\"300\" width=\"300\">\n",
    "<br>\n",
    "<img src = \"images/meme2.png\" height=\"500\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning\n",
    "\n",
    "<img src = \"images/pl.png\" height=\"400\" width=\"400\">\n",
    "\n",
    "From \"vanilla\" PyTorch, I have recently shifted to [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning), which is another great library started by [William Falcon](https://www.williamfalcon.com/), which I [quote from their docs](https://pytorch-lightning.readthedocs.io/en/stable/CONTRIBUTING.html) \"doesnâ€™t want to add any abstractions on top of pure PyTorch. This gives researchers all the control they need without having to learn yet another framework.\" It helps you reorganize your PyTorch code while providing multi-GPU and half-precision training, extensive callback system, inbuilt Tensorboard logging and a lot more on the go.\n",
    "\n",
    "Convert your PyTorch code to Lighting in 3 steps as shown [here](https://pytorch-lightning.readthedocs.io/en/stable/new-project.html). They also have an active [Youtube Channel](https://www.youtube.com/channel/UC8m-y0yAFJpX0hRvxH8wJVw) which shows how to convert your existing PyTorch code to Lightning, and also cover implementation of new papers in self-supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/fast_2.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Modern Python\n",
    "\n",
    "Go through the two links below to get upto date with Type hinting, better globbing, f-strings, data classes, using Enum for constants, and a lot more\n",
    "\n",
    "1. [An excellent post on Python 3 features](https://github.com/arogozhnikov/python3_with_pleasure) by Alex Rogozhnikov, who's also the creator of einops, the library we'll discuss next\n",
    "2. [A Twitter thread](https://twitter.com/svpino/status/1308632185113579522) by Santiago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einops: Einstein Operations on Tensors\n",
    "\n",
    "[Einops](https://github.com/arogozhnikov/einops) is one of my favorite libraries, one that gives me ASMR and one that I wish I had known while starting with PyTorch. It's written by [Alex Rogozhnikov](https://arogozhnikov.github.io) and works with all major deep learning libraries. The tutorial for using it with PyTorch: [Writing a better code with pytorch and einops](https://arogozhnikov.github.io/einops/pytorch-examples.html).\n",
    "\n",
    "<img src=\"http://arogozhnikov.github.io/images/einops/einops_video.gif\" alt=\"einops package examples\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original post is a delight to read so I'm not going to post any examples from that, I would encourage you to check it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nn.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your code becomes much cleaner and understandalbe when using nn.Sequential(). You would have already realized it by now if you have read the post linked above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "````python\n",
    "\n",
    "\n",
    "class SuperResolutionNetOld(nn.Module):\n",
    "    def __init__(self, upscale_factor):\n",
    "        super(SuperResolutionNetOld, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n",
    "        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pixel_shuffle(self.conv4(x))\n",
    "        return x\n",
    "````\n",
    "A better implementation would be\n",
    "````python\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "def SuperResolutionNetNew(upscale_factor):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(1, 64, kernel_size=5, padding=2),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(32, upscale_factor ** 2, kernel_size=3, padding=1),\n",
    "        Rearrange('b (h2 w2) h w -> b (h h2) (w w2)', h2=upscale_factor, w2=upscale_factor))\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Layers\n",
    "\n",
    "You can even use nn.Sequential() with your own torch.autograd.Function. Let's create a function for that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````python\n",
    "class Binarizer(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    An elementwise function that bins values\n",
    "    to 0 or 1 depending on a threshold of 0.5,\n",
    "    but in backward pass acts as a linear layer.\n",
    "    \n",
    "    Such layers are also known as \n",
    "    straight-through gradient estimators\n",
    "    \n",
    "    Input: a tensor with values in range (0,1)\n",
    "    Returns: a tensor with binary values: 0 or 1\n",
    "    based on a threshold of 0.5\n",
    "    Equation(1) in paper\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        return (i>0.5).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output\n",
    "\n",
    "def bin_values(x):\n",
    "    return Binarizer.apply(x)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Lambda class that acts as a wrapper\n",
    "\n",
    "````python\n",
    "class Lambda(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: A Function\n",
    "    Returns : A Module that can be used\n",
    "        inside nn.Sequential\n",
    "    \"\"\"\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x): return self.func(x)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````python\n",
    "def NewEncoder(): return nn.Sequential(nn.Conv2d(3, 128, 8, 4, 2), nn.ReLU(),\n",
    "                                       nn.Conv2d(128, 256, 4, 2, 1), nn.ReLU(),\n",
    "                                       nn.Conv2d(256, 64, 3, 1, 1), nn.Sigmoid(),\n",
    "                                       # Focus here\n",
    "                                       Lambda(bin_values))\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more naive implementation would've looked something like the one below, and that is without proper initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````python\n",
    "class OldEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        #Encoder layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=128,kernel_size=8,stride = 4,padding = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=4,stride = 2,padding = 1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256,out_channels=64,kernel_size=3,stride = 1,padding = 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))#first conv layer\n",
    "        x = F.relu(self.conv2(x))#second conv layer\n",
    "        x = torch.sigmoid(self.conv3(x))#third convolutional layer\n",
    "        x = Binarizer.apply(x)\n",
    "        return x\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to initialize neural networks and you can read more about them in my post on weight initialization [here](https://adityassrana.github.io/blog/theory/2020/08/26/Weight-Init.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
