{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "foster-morris",
   "metadata": {},
   "source": [
    "# Handling the Woes of Training\n",
    "\n",
    "> how to find the best learning rate and add OneCycle training to your deep learning model\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Aditya Rana\n",
    "- image: images/cycle_crying_kid.jpg\n",
    "- categories: [PyTorch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-structure",
   "metadata": {},
   "source": [
    "## Who needs to read this post?\n",
    "\n",
    "This post is useful if you're training deep learning models (eg. ResNets) for tasks like Image Classification, Object Detection, Image Segmentation and many more. You can use the approaches mentioned here for both training from scratch as well as for fine-tuning. This post is meant to answer questions like\n",
    "\n",
    "- what is the best learning rate for my model?\n",
    "    > <span style='color:Green'> we'll look at a learning rate finding poicy that takes less than a minute to run</span>\n",
    "\n",
    "- how to train my models much faster and spend less compute time?\n",
    "    > <span style='color:Green'> the models train faster as this approach allows us to use much higher learning rates for training that would otherwise be unsuitable </span>\n",
    "\n",
    "- how does it benefit my model?\n",
    "    > <span style='color:Green'>using higher learning rates help us avoid getting stuck in local minimas</span>\n",
    "\n",
    "- how can I schedule my learning rate to get the best performance?\n",
    "    > <span style='color:Green'> we will implement the OneCycle training policy that this post is about</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-switch",
   "metadata": {},
   "source": [
    "## TL;DR Show me the code\n",
    "\n",
    "These ideas were popularized by the [fastai](https://docs.fast.ai/) library which is based on PyTorch, but implements them using Callbacks in their custom training script. If you just want to quickly test and add LRFinder and OneCycle learning rate schedule to your training pipeline, you can directly adapt the code below to your script. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-zambia",
   "metadata": {},
   "source": [
    "### LR Range Test\n",
    "\n",
    "If you're like me, you would just put 3e-4 into an Adam optimizer and let the model train. But in the last few years, a lot has happened that has made it easier to find the optimal learning rate for our model\n",
    "\n",
    "In 2015, Leslie N. Smith came up with a trial-and-error technique called the LR Range Test. The idea is simple, you just run your model and data for a few iterations, with the learning rate initially starting at a very small value and then increasing linearly/exponentially after each iteration. We assume that the optimal learning rate is bound to lie between these two extremas, usually taken as [1e-7, 10]. You record the loss for each value of learning rate and plot it up. The low initial learning rate allows the network to start converging and as the learning rate is increased it will eventually be too large and the network will diverge. \n",
    "\n",
    "A plot for LR Range test should consist of all 3 regions, the first is where the learning rate is too small that loss barely decreases, the “just right” region where loss converges quickly, and the last region where learning rate is too big that loss starts to diverge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-assets",
   "metadata": {},
   "source": [
    "<img src = \"images/lr_range_test.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-chorus",
   "metadata": {},
   "source": [
    "For this test, you can use the library [pytorch-lr-finder](https://github.com/davidtvs/pytorch-lr-finder) for finding the best learning rate for your PyTorch model. If you are using PyTorch Lighting, you can use their builtin [lr_finder ](https://pytorch-lightning.readthedocs.io/en/latest/advanced/lr_finder.html) module. A keras implementation is also available [here](https://github.com/surmenok/keras_lr_finder). As you will see later in the post, implementing this finder is pretty straightforward once you understand the method, but I'm linking these libraries here only to give you a headstart.\n",
    "\n",
    "This is the plot we want to obtain and analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-neighborhood",
   "metadata": {},
   "source": [
    "<img src = \"images/lr_finder_lightning.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-advertising",
   "metadata": {},
   "source": [
    "**How to Interpret this**\n",
    "\n",
    "It is recommended to not pick the learning rate that achieves the lowest loss, but instead something in the middle of the sharpest downward slope (red point), as this is the point at which loss is still decreasing, whereas at the lowest point, the loss has already started increasing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-organization",
   "metadata": {},
   "source": [
    "### OneCycle Training\n",
    "\n",
    "The [OneCyle scheduler](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.OneCycleLR) is directly available for use in PyTorch. Some things to keep in mind:\n",
    "\n",
    "1. You have to call `scheduler.step()` after every batch. This is unlike most schedulers which are called after every epoch.\n",
    "\n",
    "2. OneCycle works only with optimizers that use momentum (they track the running average of gradients) like SGD, Adam and RMSProp but it won't work with AdaDelta or Adagrad which only track the running average of squared gradients. You'll understand why when we go into details.\n",
    "\n",
    "3. In my experience, Adam optimizer has worked the best with this schedule.\n",
    "\n",
    "The idea is to decrease the momentum when increasing the learning rate and to increase it when decreasing the learning rate. With this policy, the author demonstrates an event called “super-convergence”, where it reaches the same validation accuracy in only 1/5 of the iterations.\n",
    "\n",
    "<img src = \"images/lr_mom_iter.png\" width=\"700\">\n",
    "\n",
    "\n",
    "<img src = \"images/onecycle_results.png\">\n",
    "\n",
    "\n",
    "Let's get down to code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-facial",
   "metadata": {},
   "source": [
    "````python\n",
    "def get_lr(optimizer):\n",
    "    \"\"\"\"\n",
    "    for tracking how your learning rate is changing throughout training\n",
    "    \"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader):\n",
    "    history = []\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), max_lr)\n",
    "    # Set up one-cycle learning rate scheduler \n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs = epochs, \n",
    "                                                steps_per_epoch = len(train_loader))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record loss\n",
    "            train_losses.append(loss)\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        history.append(result)\n",
    "    return history\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-robert",
   "metadata": {},
   "source": [
    "If you want to see the entire training script in action, you can follow the notebooks presented below\n",
    "1. [Object Detection MNIST](https://github.com/adityassrana/object-detection-mnist)\n",
    "2. [Image Classification - CIFAR10](https://jovian.ai/aakashns/05b-cifar10-resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-burton",
   "metadata": {},
   "source": [
    "## TBD\n",
    "\n",
    "This post is a work-in-progress and will be completed in the coming days with the theoretical analysis of these ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-morocco",
   "metadata": {},
   "source": [
    "## Papers Discussed\n",
    "\n",
    "1. [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186)\n",
    "2. [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay](https://arxiv.org/abs/1803.09820)\n",
    "3. [Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-housing",
   "metadata": {},
   "source": [
    "## Extra Readings\n",
    "\n",
    "If you want to get deep (pun intended) into playing around with these concepts I would highly recommend you to watch fast.ai's [Deep Learning from the Foundations](https://course19.fast.ai/part2), spending most of your time reimplementing the notebooks by yourself. Here are some other resources that talk about these ideas\n",
    "\n",
    "1. https://sgugger.github.io/the-1cycle-policy.html\n",
    "2. https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html#how-do-you-find-a-good-learning-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-intersection",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
