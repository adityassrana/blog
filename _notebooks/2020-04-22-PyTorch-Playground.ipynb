{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Playground\n",
    "> a little-more-than-introductory guide to help people get comfortable with PyTorch functionalities\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Aditya Rana\n",
    "- image: images/pytorch_meme.png\n",
    "- categories: [tutorials]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/pytorch-logo-dark.png\" height=\"300\" width=\"300\">\n",
    "\n",
    "## Dataset and Transforms\n",
    "\n",
    "- Dataset Class : manages the data, labels and data augmentations\n",
    "- DataLoader Class : manages the size of the minibatch \n",
    "\n",
    "### Creating your Own Dataset\n",
    "\n",
    "Let's take the example of training an autoencoder in which our training data only consists of images.\n",
    "\n",
    "<img src = \"images/autoencoder_mnist.png\">\n",
    "\n",
    "The encoder can be made up of convolutional or linear layers.\n",
    "\n",
    "<img src = \"images/autoencoder.png\" height=\"400\" width=\"400\" >\n",
    "\n",
    " To create our own dataset class in PyTorch we inherit from the torch.utils.data.Dataset class and define two main methods, the ``__len__`` and the ``__getitem__``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from typing import List\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A class for creating data and augemntation pipeline\n",
    "    \"\"\"\n",
    "    def __init__(self, glob_pattern:str, patchsize:int):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        glob_pattern: this pattern must expand \n",
    "            to a list of RGB images in PNG format. \n",
    "            For eg. \"/data/train/cat/*.png\"\n",
    "            \n",
    "        patchsize: the size you want to crop\n",
    "            the image to\n",
    "\n",
    "        \"\"\"\n",
    "        self.image_paths_list = glob.glob(glob_pattern)\n",
    "        self.patchsize = patchsize\n",
    "\n",
    "    def __len__(self):\n",
    "        # denotes size of data\n",
    "        return len(self.image_paths_list)\n",
    "\n",
    "    def transform(self, image):\n",
    "        # convert to RGB if image is B/W\n",
    "        if image.mode == 'L':\n",
    "            image = image.convert('RGB')\n",
    "        self.data_transforms = transforms.Compose([transforms.RandomCrop(size = self.patchsize),\n",
    "                                                   transforms.RandomHorizontalFlip(),\n",
    "                                                   transforms.RandomVerticalFlip(),\n",
    "                                                   transforms.ToTensor()])\n",
    "        return self.data_transforms(image)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # generates one sample of data\n",
    "        image = Image.open(self.image_paths[index])\n",
    "        image= self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms\n",
    "\n",
    "Image processing operations using torchvision.transforms like cropping and resizing are done on the PIL Images and then they are converted to Tensors. The last transform which is transforms.ToTensor() seperates the the PIL Image into 3 channels (R,G,B) and scales its elements to the range (0,1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transform one observes a lot in Computer Vision based data pipelines is data normalization.\n",
    "\n",
    "````python\n",
    "transforms.Normalize(mean = [0.485, 0.456, 0.406],\n",
    "                     std = [0.229, 0.224, 0.225])\n",
    "````\n",
    "\n",
    "If you're wondering where do these mean and std values come from, the answer is, the [ImageNet dataset](http://www.image-net.org/). It's a huge dataset of 14 million images and most pre-trained models are originally trained on this. The above values are the channel-wise mean and std of all the images in the dataset. So whenever you import a pre-trained model from torchvision, make sure you apply the normalization based on the statistics of the dataset that the model was trained on. Hence, the pipeline can be summarized as\n",
    "\n",
    "    Image --> Crop/Resize --> ToTensor --> Normalize\n",
    "    \n",
    "To read more about why we normalize our data, read my blog post on this [here](https://adityassrana.github.io/blog/theory/2020/08/26/Weight-Init.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranforms functional API\n",
    "\n",
    "The functional API is stateless and you can directly pass all the necessary arguments. Whereas torchvision.transforms are classes initialized with some default parameters unless specified.\n",
    "\n",
    "````python\n",
    "# Class-based. Define once and use multiple times\n",
    "transform = transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "data = transform(data)\n",
    "\n",
    "# Functional. Pass parameters each time\n",
    "data = TF.normalize(data, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functional API is very useful when transforming your data and target with the same random values, e.g. random cropping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````python\n",
    "import torchvision.transforms.functional as TF #it's not tensorflow :p\n",
    "i, j, h, w = transforms.RandomCrop.get_params(image, output_size=(512, 512))\n",
    "image = TF.crop(image, i, j, h, w)\n",
    "mask = TF.crop(mask, i, j, h, w)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also allows us to perform identical transforms on both image and target\n",
    "````python\n",
    "def transform(self, image, mask):\n",
    "    # Resize\n",
    "    resize = transforms.Resize(size=(520, 520))\n",
    "    image = resize(image)\n",
    "    mask = resize(mask\n",
    "\n",
    "# Random horizontal flipping\n",
    "if random.random() > 0.5:\n",
    "    image = TF.hflip(image)\n",
    "    mask = TF.hflip(mask)\n",
    "\n",
    "# Random vertical flipping\n",
    "if random.random() > 0.5:\n",
    "    image = TF.vflip(image)\n",
    "    mask = TF.vflip(mask)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders\n",
    "\n",
    "The data is passed to the model few samples at a time as datasets are usually too big to fit entirely on the CPU/GPU. \n",
    "\n",
    "For choosing an appropriate batch_size, make it as high as possible as long as you dont encounter `RuntimeError: CUDA out of memory` and as long as it's a multiple of 16.\n",
    "\n",
    "````python\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = 32,\n",
    "                          shuffle=True, \n",
    "                          num_workers = 4)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation, where does it happen?\n",
    "\n",
    "A lot of people get confused about how data augmentation helps in increasing the size of the dataset when we're not actually creating or saving new images. The point to understand here is that data augmentation happens on the fly. Every time `__getitem__` method in the Dataset Class is called by the DataLoader, the transformations are applied. \n",
    "\n",
    "When you  use the dataloader in your training loop, at the start of every epoch it supplies a new data-augemnted dataset with the augmentations applied to each element. This means at each epoch, the model will see a new variant of the dataset.\n",
    "\n",
    "````python\n",
    "for epoch in range(epochs):\n",
    "    for data in train_loader():\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kornia\n",
    "\n",
    "Another thing to note is that these operations are performed on the CPU so you need to make sure that your data processing does not become your training bottleneck when using large batchsizes. This is the time for introducing - \n",
    "\n",
    "<img src = \"images/kornia_logo.svg\" height=\"300\" width=\"300\" >\n",
    "\n",
    "[Kornia](https://github.com/kornia/kornia) is a differentiable computer vision library for PyTorch that operates directly on tensors, hence letting you make full use of your GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Writing Custom Autograd Functions / Layers\n",
    "\n",
    "### Writing your own ReLU\n",
    "\n",
    "````python\n",
    "class MyReLU(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        \"\"\" \n",
    "        ctx is a context object that can be used\n",
    "        to stash information for backward computation. \n",
    "        You can cache arbitrary objects for use in the \n",
    "        backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        input = i.clone()\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor \n",
    "        containing the gradient of the loss wrt \n",
    "        the output, and we need to compute the gradient \n",
    "        of the loss wrt the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Gradient Shape\n",
    "\n",
    "<img src = \"images/local_gradients_backprop.png\" height=\"500\" width=\"500\">\n",
    "\n",
    "Gradient returned by the backward method of the class should have the same shape as the input to the forward method of the class, so that the gradient computed for the input after the loss.backward() step has the same shape as input and can be used to update it in the optimizer.step()\n",
    "\n",
    "loss.backward() computes d(loss)/d(w) for every parameter which has requires_grad=True. They are accumulated in w.grad. And the optimizer.step() updates w using w.grad, w += -lr* x.grad\n",
    "\n",
    "For more info read the posts below\n",
    "\n",
    "- [PyTorch Custom Layers](https://adityassrana.github.io/blog/programming/pytorch/2020/09/25/Modern-PyTorch.html#Custom-Layers)\n",
    "- [PyTorch Source Code Examples on Github](https://github.com/pytorch/pytorch/blob/53fe804322640653d2dddaed394838b868ce9a26/torch/autograd/_functions/pointwise.py)\n",
    "- [PyTorch official docs](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html)\n",
    "\n",
    "Avoid using in-place operations as they cause problems while back-propagation because of the way they modify the graph. As a precaution, always clone the input in the forward pass, and clone the incoming gradients before modifying them.\n",
    "\n",
    "An in-place operation directly modifies the content of a given Tensor without making a copy. Inplace operations in PyTorch are always postfixed with a _, like .add_() or .scatter_(). Python operations like \\+ = or \\*= are also in-place operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with non-differentiable functions\n",
    "\n",
    "Sometimes in your model or loss calculation you need to use functions that are non-differentiable. For calculating gradients, autograd requires all components of the graph to be differentiable. You can work around this by using a proxy function in the backward pass calculations.\n",
    "\n",
    "    f_hard : non-differentiable\n",
    "    f_soft : differentiable proxy for w_hard\n",
    "    \n",
    "````python\n",
    "f_out = f_soft + (f_hard - f_soft).detach()  # in PyTorch\n",
    "f_out = f_soft + tf.stop_grad(f_hard - f_soft) # in Tensorflow\n",
    "````\n",
    "### Core Idea\n",
    "\n",
    "````\n",
    "y = x_backward + (x_forward - x_backward).detach()\n",
    "````\n",
    "It gets you x_forward in the forward pass, but derivative acts as if you had x_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````python\n",
    "class Binarizer(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    An elementwise function that bins values\n",
    "    to 0 or 1 depending on a threshold of 0.5,\n",
    "    but in backward pass acts as an identity layer.\n",
    "    \n",
    "    Such layers are also known as \n",
    "    straight-through gradient estimators\n",
    "    \n",
    "    Input: a tensor with values in range (0,1)\n",
    "    Returns: a tensor with binary values: 0 or 1\n",
    "    based on a threshold of 0.5\n",
    "    Equation(1) in paper\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        return (i>0.5).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output\n",
    "\n",
    "def bin_values(x):\n",
    "    return Binarizer.apply(x)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function can be reimplemented with a single line in Pytorch\n",
    "while maintaining differentiabilty\n",
    "\n",
    "````python\n",
    "def bin_values(x):\n",
    "    return x + ((x>0.5).float() - x).detach()\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Training and  Validation Loop\n",
    "\n",
    "````python\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        # Handle batchnorm / dropout\n",
    "        model.train()\n",
    "        # print(model.training)\n",
    "        for mini_batch in train_dl:\n",
    "            pred = model(mini_batch)\n",
    "            loss = loss_func(pred, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        #print(model.training)\n",
    "        with torch.no_grad():\n",
    "            for mini_batch in valid_dl:\n",
    "                pred = model(mini_batch)\n",
    "                # log some metrics here\n",
    "            # aggregate metrics from all batches\n",
    "````\n",
    "\n",
    "Once you become more familiar with writing training and validation loops, I would recommend you to try out PyTorch Lightning [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) , which is a great library started by [William Falcon](https://www.williamfalcon.com/) that helps you get rid of all the PyTorch boilerplate code and instead lets you focus on the research part of your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "### Installing\n",
    "Install tensorboard with `pip install tensorboard`\n",
    "\n",
    "<img src = \"images/tensorboard.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a SummaryWriter\n",
    "\n",
    "````python\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer_train = SummaryWriter(os.path.join(args.experiment_dir,\"tensorboard\"))\n",
    "````\n",
    "\n",
    "### Scalars\n",
    "\n",
    "Logging statements are added at different steps in the training loop wherever you want to log something. You can track scalars, images and even histograms. You can read more about this on the official [PyTorch docs](https://pytorch.org/docs/stable/tensorboard.html)\n",
    "\n",
    "Logging scalars can be as simple as\n",
    "\n",
    "````python\n",
    "writer_train.add_scalar('train_loss', loss.item(), iteration)\n",
    "````\n",
    "where iteration is the global_step_count that you can keep track of inside your training loop.\n",
    "\n",
    "### Images\n",
    "\n",
    "We'll use `make_grid` to create a grid of images directly from tensors so that we can plot them together.\n",
    "\n",
    "````python\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# x is a tensor of Images of the shape (N,3,H,W)\n",
    "x_grid = make_grid(x[:5],nrow=5)\n",
    "writer_train.add_image('train/original_images',x_grid, iteration)\n",
    "````\n",
    "### Launch\n",
    "\n",
    "To visualize what you've logged, launch a tensorboard instance from the terminal by entering `tensorboard --logdir .` in the directory where you have logged your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "To make predictions out of your trained model, make sure you feed data in the right format.\n",
    "\n",
    "Input Tensor Format : (batch_size, channels, height, width). The model and the convolutional layers expect the input tensor to be of the shape (N,C,H,W), so when feeding an image/images to the model, add a dimension for batching.\n",
    "\n",
    "Converting from img-->numpy representation and feeding the model gives an error because the input is in ByteTensor format. Only float operations are supported for conv-like operations. So add an extra step after numpy conversion - \n",
    "\n",
    "````py\n",
    "img = img.type('torch.DoubleTensor')\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models\n",
    "\n",
    "PyTorch saves a model as a state_dict and the extension used is .pt\n",
    "\n",
    "````py\n",
    "torch.save(model.state_dict(), PATH = 'latest_checkpoint.pt')\n",
    "````\n",
    "\n",
    "Sometimes you add new layers to your model which which were not present in the model you saved as checkpoint. In such a case set the `strict` keyword to False\n",
    "\n",
    "````python\n",
    "model = Model()\n",
    "checkpoint = torch.load('latest_checkpoint.pt')\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "````\n",
    "\n",
    "On Loading a model, if it shows a message like this, it means there were no missing keys and everything went well ( it's not an error ).\n",
    "\n",
    "````\n",
    "IncompatibleKeys(missing_keys=[], unexpected_keys=[])\n",
    "````\n",
    "\n",
    "Keyboard interrupt and saving the last state of a model if you need to stop the experiment mid-way of training:\n",
    "\n",
    "````python\n",
    "try:\n",
    "    # training code here\n",
    "except KeyboardInterrupt:\n",
    "    # save model here\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Resources\n",
    "\n",
    "- [Grokking PyTorch](https://github.com/Kaixhin/grokking-pytorch/blob/master/README.md)\n",
    "- [Effective PyTorch](https://github.com/vahidk/EffectivePyTorch/blob/master/README.md)\n",
    "- [The Python Magic Behind PyTorch](https://amitness.com/2020/03/python-magic-behind-pytorch)\n",
    "- [Python is Cool - ChipHuyen](https://github.com/chiphuyen/python-is-cool/blob/master/README.md)\n",
    "- [PyTorch StyleGuide](https://github.com/IgorSusmelj/pytorch-styleguide/blob/master/README.md)\n",
    "- [Clean Code Python](https://github.com/zedr/clean-code-python)\n",
    "- [Using _ in Variable Naming](https://dbader.org/blog/meaning-of-underscores-in-python)\n",
    "- [Pytorch Coding Conventions](https://discuss.pytorch.org/t/pytorch-coding-conventions/42548)\n",
    "- [Fine Tuning etc](https://spandan-madan.github.io/A-Collection-of-important-tasks-in-pytorch/)\n",
    "- [https://github.com/dsgiitr/d2l-pytorch](https://github.com/dsgiitr/d2l-pytorch)\n",
    "- [https://github.com/L1aoXingyu/pytorch-beginner](https://github.com/L1aoXingyu/pytorch-beginner)\n",
    "- [https://github.com/yunjey/pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial)\n",
    "- [https://github.com/MorvanZhou/PyTorch-Tutorial](https://github.com/MorvanZhou/PyTorch-Tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
