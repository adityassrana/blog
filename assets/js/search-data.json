{
  
    
        "post0": {
            "title": "Handling the Woes of Training",
            "content": "Who needs to read this post? . This post is useful if you&#39;re training deep learning models (eg. ResNets) for tasks like Image Classification, Object Detection, Image Segmentation and many more. You can use the approaches mentioned here for both training from scratch as well as for fine-tuning. This post is meant to answer questions like . what is the best learning rate for my model? we&#39;ll look at a learning rate finding policy that takes less than a minute to run . | how to train my models much faster and spend less compute time? the models train faster as this approach allows us to use much higher learning rates for training that would otherwise be unsuitable . | how does it benefit my model?using higher learning rates help us avoid getting stuck in local minimas . | how can I schedule my learning rate to get the best performance? we will implement the OneCycle training policy that this post is about . | . TL;DR Show me the code . These ideas were popularized by the fastai library which is based on PyTorch, but implements them using Callbacks in their custom training script. If you just want to quickly test and add LRFinder and OneCycle learning rate schedule to your training pipeline, you can directly adapt the code below to your script. . LR Range Test . If you&#39;re like me, you would just put 3e-4 into an Adam optimizer and let the model train. But in the last few years, a lot has happened that has made it easier to find the optimal learning rate for our model . In 2015, Leslie N. Smith came up with a trial-and-error technique called the LR Range Test. The idea is simple, you just run your model and data for a few iterations, with the learning rate initially starting at a very small value and then increasing linearly/exponentially after each iteration. We assume that the optimal learning rate is bound to lie between these two extremas, usually taken as [1e-7, 10]. You record the loss for each value of learning rate and plot it up. The low initial learning rate allows the network to start converging and as the learning rate is increased it will eventually be too large and the network will diverge. . A plot for LR Range test should consist of all 3 regions, the first is where the learning rate is too small that loss barely decreases, the “just right” region where loss converges quickly, and the last region where learning rate is too big that loss starts to diverge. . . For this test, you can use the library pytorch-lr-finder for finding the best learning rate for your PyTorch model. If you are using PyTorch Lighting, you can use their builtin lr_finder module. A keras implementation is also available here. As you will see later in the post, implementing this finder is pretty straightforward once you understand the method, but I&#39;m linking these libraries here only to give you a headstart. . This is the plot we want to obtain and analyze . . How to Interpret this . It is recommended to not pick the learning rate that achieves the lowest loss, but instead something in the middle of the sharpest downward slope (red point), as this is the point at which loss is still decreasing, whereas at the lowest point, the loss has already started increasing. . OneCycle Training . The OneCyle scheduler is directly available for use in PyTorch. Some things to keep in mind: . You have to call scheduler.step() after every batch. This is unlike most schedulers which are called after every epoch. . | OneCycle works only with optimizers that use momentum (they track the running average of gradients) like SGD, Adam and RMSProp but it won&#39;t work with AdaDelta or Adagrad which only track the running average of squared gradients. You&#39;ll understand why when we go into details. . | In my experience, Adam optimizer has worked the best with this schedule. . | The idea is to decrease the momentum when increasing the learning rate and to increase it when decreasing the learning rate. With this policy, the author demonstrates an event called “super-convergence”, where it reaches the same validation accuracy in only 1/5 of the iterations. . . . Let&#39;s get down to code . def get_lr(optimizer): &quot;&quot;&quot;&quot; for tracking how your learning rate is changing throughout training &quot;&quot;&quot; for param_group in optimizer.param_groups: return param_group[&#39;lr&#39;] def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader): history = [] # Set up optimizer optimizer = torch.optim.SGD(model.parameters(), max_lr) # Set up one-cycle learning rate scheduler sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs = epochs, steps_per_epoch = len(train_loader)) for epoch in range(epochs): # Training Phase model.train() train_losses = [] lrs = [] for batch in train_loader: loss = model.training_step(batch) loss.backward() optimizer.step() optimizer.zero_grad() # Record loss train_losses.append(loss) # Record &amp; update learning rate lrs.append(get_lr(optimizer)) sched.step() # Validation phase with torch.no_grad(): model.eval() result = evaluate(model, val_loader) result[&#39;train_loss&#39;] = torch.stack(train_losses).mean().item() result[&#39;lrs&#39;] = lrs history.append(result) return history . If you want to see the entire training script in action, you can follow the notebooks presented below . Object Detection MNIST | Image Classification - CIFAR10 | Papers Discussed . Cyclical Learning Rates for Training Neural Networks | A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay | Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates | Extra Readings . If you want to get deep (pun intended) into playing around with these concepts I would highly recommend you to watch fast.ai&#39;s Deep Learning from the Foundations, spending most of your time reimplementing the notebooks by yourself. Here are some other resources that talk about these ideas . https://sgugger.github.io/the-1cycle-policy.html | https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html#how-do-you-find-a-good-learning-rate |",
            "url": "https://adityassrana.github.io/blog/pytorch/2021/03/31/OneCycle-Training.html",
            "relUrl": "/pytorch/2021/03/31/OneCycle-Training.html",
            "date": " • Mar 31, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Writing Modern PyTorch",
            "content": "PyTorch . . Learning a programming language/framework is a lot like learning foreign languages. Just studying them online or from a book is hardly sufficient and one needs to get actively involved in conversations and discussions to get the pronunciation and flow of speaking right. The same goes for writing code, so get started on the PyTorch Forums and Stack Overflow. . I&#39;m writing this post after 2 years of using PyTorch, after having started learning from Udacity courses and online blogs, to heavily experimenting with PyTorch&#39;s functionalities during my bachelor&#39;s thesis, and then more recently having finished the fast.ai&#39;s Deep Learning from Foundations where Jeremy Howard recreates several core modules of PyTorch and discusses his thought process on creating the latest fastai library. . The purpose of this post is to not to be an all-purpose tutorial or template since there&#39;re already a lot of amazing people out there teaching PyTorch, but instead it aims to answer the FAQs and guide people to the right resources. . Getting Started . . If I had to start learning PyTorch all over again, I wouldn&#39;t think twice and dig deep into everything Jeremy Howard has to offer. He is the co-founder of fast.ai along with Rachel Thomas and every year they release several courses on deep learning for free. This is world-class educational content you can enjoy at no cost, not even any ads or sponsors * gasps *. There aren&#39;t many quality things in the world that come for free so I would definitely recommend you to check fast.ai out. . If you&#39;re confused where to start from all the courses offered, I would suggest watching the first 2-3 videos of their most recent offering of &quot;Practical Deep Learning for Coders&quot;, and in parallel starting with the course Deep Learning from Foundations . If you&#39;re looking for a fast-track introduction to PyTorch, you can read this tutorial &quot;What is torch.nn really?&quot; by Jeremy on the official PyTorch page. . Educational vs Practical . At this point, I must also point out that I have sort of a love-hate relationship with fastai owing to some aspects of their coding style. Even though I&#39;ve been bluntly advertising them since the start of this post, I do not use the fastai library so often for my projects. I think fastai is the best educational library out there which will get you SOTA results for tasks like Image Classification, Segmentation and a bunch of other tasks in less than 10 line of code, but the fact that instead of developing their library around existing PyTorch functionalities and supporting them, they have tried to create their own abstractions by rewriting PyTorch modules like DataLoaders and introduced their own optimizers, without providing enough extra utility to balance the trade-off. . What I do use regularly from fastai is a tonne of ideas that I learned while doing their courses like proper weight initialization, learning rate finder, OneCycle training policy, callbacks, PyTorch Hooks and visualizing layer histograms, just to name a few. . PyTorch vs Keras/TF . Sometimes you would see people fighting over PyTorch vs Keras/Tensorflow and which is better. I usually don&#39;t enjoy such debates, or even support PyTorch for that matter. I believe there&#39;s a variety of people out there who have their own programming style preferences, and whichever framework suits their taste better, they should use it. I feel more comfortable and cognitively at ease using PyTorch and that&#39;s why I prefer the PyTorch ecosystem, but at the same time I don&#39;t mind working with Tensorflow whenever I have to. . . I do like PyTorch vs TF memes though, who doesn&#39;t? . . Lightning . . From &quot;vanilla&quot; PyTorch, I have recently shifted to PyTorch Lightning, which is another great library started by William Falcon, which I quote from their docs &quot;doesn’t want to add any abstractions on top of pure PyTorch. This gives researchers all the control they need without having to learn yet another framework.&quot; It helps you reorganize your PyTorch code while providing multi-GPU and half-precision training, extensive callback system, inbuilt Tensorboard logging and a lot more on the go. . Convert your PyTorch code to Lighting in 3 steps as shown here. They also have an active Youtube Channel which shows how to convert your existing PyTorch code to Lightning, and also cover implementation of new papers in self-supervised learning. . . Writing Modern Python . You should go through this excellent post on Python3 features by Alex Rogozhnikov where he discusses type hinting, better globbing, f-strings, data classes, using Enum for constants, and a lot more. He is also the creator of einops, the library we&#39;ll discuss next . Einops: Einstein Operations on Tensors . Einops is one of my favorite libraries, one that gives me ASMR and one that I wish I had known while starting with PyTorch. It&#39;s written by Alex Rogozhnikov and works with all major deep learning libraries. The tutorial for using it with PyTorch: Writing a better code with pytorch and einops. . The original post is a delight to read so I&#39;m going to post only one example from that and not much. . Using nn.Sequential() . your code becomes much cleaner and understandable when using nn.Sequential(). You would have already realized it by now if you read the post linked above. . class SuperResolutionNetOld(nn.Module): def __init__(self, upscale_factor): super(SuperResolutionNetOld, self).__init__() self.relu = nn.ReLU() self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2)) self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1)) self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1)) self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1)) self.pixel_shuffle = nn.PixelShuffle(upscale_factor) def forward(self, x): x = self.relu(self.conv1(x)) x = self.relu(self.conv2(x)) x = self.relu(self.conv3(x)) x = self.pixel_shuffle(self.conv4(x)) return x . A better implementation would be . from einops.layers.torch import Rearrange def SuperResolutionNetNew(upscale_factor): return nn.Sequential( nn.Conv2d(1, 64, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(64, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(32, upscale_factor ** 2, kernel_size=3, padding=1), Rearrange(&#39;b (h2 w2) h w -&gt; b (h h2) (w w2)&#39;, h2=upscale_factor, w2=upscale_factor)) . Custom Layers . You can even use nn.Sequential() with your own torch.autograd.Function. Let&#39;s create a function for that . class Binarizer(torch.autograd.Function): &quot;&quot;&quot; An elementwise function that bins values to 0 or 1 depending on a threshold of 0.5, but in backward pass acts as an identity layer. Such layers are also known as straight-through gradient estimators Input: a tensor with values in range (0,1) Returns: a tensor with binary values: 0 or 1 based on a threshold of 0.5 Equation(1) in paper &quot;&quot;&quot; @staticmethod def forward(ctx, i): return (i&gt;0.5).float() @staticmethod def backward(ctx, grad_output): return grad_output def bin_values(x): return Binarizer.apply(x) . Creating a Lambda class that acts as a wrapper . class Lambda(nn.Module): &quot;&quot;&quot; Input: A Function Returns : A Module that can be used inside nn.Sequential &quot;&quot;&quot; def __init__(self, func): super().__init__() self.func = func def forward(self, x): return self.func(x) . def NewEncoder(): return nn.Sequential(nn.Conv2d(3, 128, 8, 4, 2), nn.ReLU(), nn.Conv2d(128, 256, 4, 2, 1), nn.ReLU(), nn.Conv2d(256, 64, 3, 1, 1), nn.Sigmoid(), # Focus here Lambda(bin_values)) . A more naive implementation would&#39;ve looked something like the one below, and that is without proper initialization. . class OldEncoder(nn.Module): def __init__(self): super(Encoder, self).__init__() #Encoder layers self.conv1 = nn.Conv2d(in_channels=3,out_channels=128,kernel_size=8,stride = 4,padding = 2) self.conv2 = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=4,stride = 2,padding = 1) self.conv3 = nn.Conv2d(in_channels=256,out_channels=64,kernel_size=3,stride = 1,padding = 1) def forward(self,x): x = F.relu(self.conv1(x))#first conv layer x = F.relu(self.conv2(x))#second conv layer x = torch.sigmoid(self.conv3(x))#third convolutional layer x = Binarizer.apply(x) return x . There are several ways to initialize neural networks and you can read more about them in my post on weight initialization here . Kornia . Data Augmentation . Data Augmentation in PyTorch pipelines is usually done using torchvision.transforms. The pipeline can be summarized as . Image --&gt; Crop/Resize --&gt; ToTensor --&gt; Normalize . All the augmentattions are performed on the CPU so you need to make sure that your data processing does not become your training bottleneck when using large batchsizes. This is the time for introducing - . . Kornia is a differentiable computer vision library for PyTorch started by Edgar Riba and Dmytro Mishkin, that operates directly on tensors, hence letting you make full use of your GPUs. They have also recently released a paper . It allows you to use data augmentation similar to a nn.Module(), and you can even combine the transforms in a nn.Sequential() . import kornia transform = nn.Sequential( kornia.enhance.AdjustBrightness(0.5), kornia.enhance.AdjustGamma(gamma=2.), kornia.enhance.AdjustContrast(0.7), ) images = transform(images) . These are the most important things you need to know . Features . . Comparison with Other Pipelines . . .",
            "url": "https://adityassrana.github.io/blog/programming/pytorch/2020/09/25/Modern-PyTorch.html",
            "relUrl": "/programming/pytorch/2020/09/25/Modern-PyTorch.html",
            "date": " • Sep 25, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Don't Trust PyTorch to Initialize Your Variables",
            "content": "This post is a summary of things I learnt while doing Fast.ai&#39;s Deep Learning from Foundations and Justin Johnsons&#39;s Computer Vision Course at UMichigan which is the updated and latest version of CS231n. . Situation . You want to solve a problem using deep learning. You have collected a dataset, decided on a neural network architecture, loss function, optimizer and some metrics you want to evaluate on. While training you notice your network isn&#39;t performing well, neither on train nor validation dataset. Looking for bugs while training neural networks is not a simple task, so we break down the whole training process into separate pipelines. Let&#39;s start by looking for bugs in our architecture and the way we initialize our weights. . Problem: Why does good initialization matter? . As Neural Networks involve a lot of matrix multiplications, the mean and variance of activations can quickly shoot off to very high values or drop down to zero. This will cause the local gradients of our layers to become NaN or zero and hence prevent our network from learning anything . A common strategy to avoid this is to initialize the weights of your network using the latest techniques. For example if you’re using ReLU activation after a layer, you must initialize your weights with Kaiming He initialization and set the biases to zero.(This was introduced in the 2014 ImageNet winning paper from Microsoft). This ensures the mean and standard deviation of activations of all layers stay close to 0 and 1 respectively. . . Upstream gradients are multiplied by local gradients to get the downstream gradients during backpropLet&#39;s compare the local gradient behavior of some common activation functions . . Notice how the gradients in both sigmoid and tanh are non-zero only inside a certain range between [-5, 5] . Also notice that when using sigmoid, the local gradient achieves a maximum value of 0.25, thus every time gradient passes through a sigmoid layer, it gets diminished by at least 75%. . Task . To ensure the gradients of our network do not explode or diminish to zero . The Mean of activations should be zero | The Variance of activations should stay same across layers. | Intitalizing the network with the right weights can make the difference between converging in a few epochs versus not converging at all. . Solution: Let&#39;s Compare Differrent Initialization Strategies . You must be wondering that surely it cannot be that bad. Let&#39;s consider a forward pass for a 6-layer neural network with each hidden layer the size of 4096 and tanh activation and let&#39;s plot the histogram for activations after each layer . All Weights and Biases Set to Zero . . Image SourceDo not ever do this! In such a case, all neurons of a layer would end up computing the same output, will receive the same gradients during backpropagation and undergo the same parameter updates. That means that all neurons of a layer will be learning the same features during training as there is no symmetry breaking. Also The problem with zero initialized layer is that since its output will be zero, it will halt the gradient flow at the next layer by making the local gradients of weights for the next layer zero. . import numpy as np import matplotlib.pyplot as plt %matplotlib inline . plt.style.use(&#39;seaborn&#39;) . Using Small Random Numbers from a Normal Distribution . Why Normal/Gaussian? Because it is characterized by its mean and variance, exactly the two things we want to control and compare . dims = [4096]*7 mean = [] std = [] hist = [] x = np.random.randn(dims[0]) for Din, Dout in zip(dims[:-1],dims[1:]): #Focus here W = 0.01*np.random.randn(Din,Dout) x = np.tanh(x.dot(W)) mean.append(x.mean()) std.append(x.std()) hist.append(x) fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 8)) for i,ax,m,s in zip(np.arange(6),axes.flatten(),mean,std): ax.hist(hist[i],density = True,bins=50) ax.set_xlim(-1.2,1.2) ax.set_title(f&#39;Layer:{i+1} mean:{m:.2f} and std;{s:.2f}&#39;) plt.tight_layout() . Why and when do gradients vanish? . As all activations tend to zero for deeper layers, the gradients are going to collapse to zero as well.Why? . . Because whenever we compute local gradient on the weight, the local gradient of the weight is going to be equal to to the activation of the previous layer. So this means for a deep network, if the activations before a layer collapse to zero, the local gradients on the weight collapse as well and the network will stop learning. That&#39;s why for gradients to stop vanishing or exploding it&#39;s important to prevent activations from vanishing or exploding respectively. . . Backprop for a Linear Layer . If this part is unclear to you, you should revise backpropagation for a linear layer by having a look at this post by Justin Johnson. For extra resources you can also have a look at . http://cs231n.stanford.edu/handouts/linear-backprop.pdf | http://cs231n.stanford.edu/handouts/derivatives.pdf | https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf | https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture5.pdf | fig, ax = plt.subplots(figsize=(3,3)) ax.hist(hist[5],bins=50) ax.set_title(f&#39;Layer:{6} mean:{m:.2f} and std;{s:.2f}&#39;) ax.set_xlim(-1,1) plt.tight_layout() . As we can see, most of the activations in the last layer are around zero. . The same activations as above super-imposed on each other. Plotting this just because it seems visually appealing to me . plt.style.use(&#39;default&#39;) for m,s in zip(mean,std): a = np.random.normal(m,s,size=1000) plt.hist(a,bins=50) . plt.style.use(&#39;seaborn&#39;) . Maybe larger weights will not get diminished . dims = [4096]*7 mean = [] std = [] hist = [] x = np.random.randn(dims[0]) for Din, Dout in zip(dims[:-1],dims[1:]): W = 0.5*np.random.randn(Din,Dout) x = np.tanh(x.dot(W)) mean.append(x.mean()) std.append(x.std()) hist.append(x) fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 8)) for i,ax,m,s in zip(np.arange(6),axes.flatten(),mean,std): ax.hist(hist[i],density = True,bins=50) ax.set_xlim(-1.2,1.2) ax.set_title(f&#39;Layer:{i+1} mean:{m:.2f} and std;{s:.2f}&#39;) plt.tight_layout() . Wow, that was a disaster. All neurons are saturated and are outputting -1s and 1s where gradient is zero. Again, no learning . Xavier : Magic Scaling Number $$ sqrt{ frac{1}{Din}}$$ . dims = [4096]*7 mean = [] std = [] hist = [] x = np.random.randn(dims[0]) for Din, Dout in zip(dims[:-1],dims[1:]): W = np.random.randn(Din,Dout)/np.sqrt(Din) x = np.tanh(x.dot(W)) mean.append(x.mean()) std.append(x.std()) hist.append(x) fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 8)) for i,ax,m,s in zip(np.arange(6),axes.flatten(),mean,std): ax.hist(hist[i],density = True,bins=50) ax.set_xlim(-1.2,1.2) ax.set_title(f&#39;Layer:{i+1} mean:{m:.2f} and std;{s:.2f}&#39;) plt.tight_layout() . Hmmmmmmmmm, why did that work? This was proposed in Understanding the difficulty of training deep feedforward neural networks, the 2010 paper that introduced ‘Xavier initialization’ which we have just used. . But where does it come from? It&#39;s not that mysterious if you remember the definition of the matrix multiplication. When we do y = x @ W, the coefficients of y are defined by . $$y_{i} = x_{1} w_{1} + x_{2} w_{2} + cdots + x_{n} w_{n} = sum_{j=1}^{Din} x_{j} w_{j} $$ . Now at the very beginning, our x vector has a mean of roughly 0. and a standard deviation of roughly 1. (since we picked it that way). . $$ Var(y_{i}) = Var(x_{1} w_{1} + x_{2} w_{2} + cdots + x_{n} w_{n}) $$ . Assume x, w are iid $$ Var(y_{i}) = D_{in} * Var(x_{i} w_{i}) $$ | Assume x, w independent $$ Var(y_{i}) = D_{in} * (E[x_{i}^2] E[w_{i}^2] - E[x_{i}]^2 E[w_{i}]^2) $$ | Assume x, w are zero-mean $$ Var(y_{i}) = D_{in} * Var(x_{i}) * Var(w_{i}) $$ | Hence for the variances to remain same between layers that is $$ Var(y_{i}) = Var(x_{i})$$ the condition is $$ Var(w_i) = frac{1}{Din} $$ . That&#39;s it, the Xavier initialization. . Kaiming Initialization, if using ReLU scale by $$ sqrt{ frac{2}{Din}}$$ . Xavier initialization assumes the input to have zero mean, but things change when we use a ReLU which sets all negative values to zero. Let&#39;s see what happens if we continue using Xavier initialization with ReLU . dims = [4096]*7 mean = [] std = [] hist = [] x = np.random.randn(dims[0]) for Din, Dout in zip(dims[:-1],dims[1:]): W = np.random.randn(Din,Dout)/np.sqrt(Din) x = np.maximum(0, x.dot(W)) print(x.shape) mean.append(x.mean()) std.append(x.std()) hist.append(x) fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 8)) for i,ax,m,s in zip(np.arange(6),axes.flatten(),mean,std): ax.hist(hist[i],density = True,bins=50) ax.set_xlim(-1.2,1.5) ax.set_title(f&#39;Layer:{i+1} mean:{m:.2f} and std;{s:.2f}&#39;) plt.tight_layout() . (4096,) (4096,) (4096,) (4096,) (4096,) (4096,) . Things aren&#39;t looking too good as most activations are again driven to zero on reaching the last layer. Let&#39;s try out Kaiming init now . dims = [4096]*7 mean = [] std = [] hist = [] x = np.random.randn(dims[0]) for Din, Dout in zip(dims[:-1],dims[1:]): W = np.random.randn(Din,Dout)/np.sqrt(Din/2) x = np.maximum(0, x.dot(W)) mean.append(x.mean()) std.append(x.std()) hist.append(x) fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 8)) for i,ax,m,s in zip(np.arange(6),axes.flatten(),mean,std): ax.hist(hist[i],density = True,bins=50) ax.set_xlim(-1.2,1.5) ax.set_title(f&#39;Layer:{i+1} mean:{m:.2f} and std;{s:.2f}&#39;) plt.tight_layout() . The activations look much better now . Here&#39;s a sample code if you get confused how to add multiple plots to the same figure. You can read more about this here . #collapse-hide xs = np.linspace(0, 2 * np.pi, 400) ys = np.sin(xs ** 2) xc = np.linspace(0, 2 * np.pi, 600) yc = np.cos(xc ** 2) fig, ax = plt.subplots(2, 2, figsize=(16, 8)) # `Fig` is short for Figure. `ax` is short for Axes. ax[0, 0].plot(xs, ys) ax[1, 1].plot(xs, ys) ax[0, 1].plot(xc, yc) ax[1, 0].plot(xc, yc) fig.suptitle(&quot;Basic plotting using Matplotlib&quot;) plt.show() . . How to calculate fan-in and fan-out in Xavier initialization for CNNs? . While reading the papers on initialization you&#39;ll come across these two terms &#39;fan-in&#39; and &#39;fan-out&#39; quite often. This part of my post is inspired by Matthew Kleinsmith&#39;s post on CNN Visualizations on Medium. . Let&#39;s start by taking a dense layer as shown below that connects 4 neurons with 6 neurons. The dense layer has a shape of [4x6] (or [6x4] depending on how you implement matrix multiplication). . The neurons themselves are often referred to as layers. It&#39;s common to read the below architecture as having an input layer of 4 neurons and output layer of 6 neurons. Do not get confused by this terminology. There is only one layer here - the dense layer which transforms an input of 4 features to 6 features by multiplying it with a weight matrix. We want to calculate fan_in and fan_out for correct initialization of this weight matrix. . fan_in is the number of inputs to a layer (4) | fan_out is the number of outputs to a layer (6) | . . The above image was generated using this wonderful tool by Alexander Lenail. . &gt; &gt;&gt; from torch import nn &gt;&gt;&gt; linear = nn.Linear(4,6) &gt;&gt;&gt; print(nn.init._calculate_fan_in_and_fan_out(linear.weight)) (4, 6) . Similarly, a Conv Layer can be visualized as a Dense(Linear) layer. . . The Image . The Filter . Since the filter fits in the image four times, we have four results . Here’s how we applied the filter to each section of the image to yield each result . The equation view . The compact equation viewand now most importantly the neural network view where you can see each output is generated from 4 inputs and hence fan_in = 4. . . If the original image had been a 3-channel image, each output would be generated from 3*4 = 12 inputs and hence fan_in would be 12. Hence, . receptive_field_size = kernel_height * kernel_width fan_in = num_input_feature_maps * receptive_field_size fan_out = num_output_feature_maps * receptive_field_size . I would also encourage you to play around with the PyTorch functions for calculating fan_in and fan_out here.. . import torch conv = torch.nn.Conv2d(in_channels=1,out_channels=1,kernel_size=2) print(f&#39;Conv shape: {conv.weight.shape}&#39;) . Conv shape: torch.Size([1, 1, 2, 2]) . #returns fan_in and fan_out value for a layer torch.nn.init._calculate_fan_in_and_fan_out(conv.weight) . (4, 4) . print(f&#39;number of elements: {conv.weight.numel()}&#39;) print(f&#39;mean: {conv.weight.mean()} and std: {conv.weight.std()} &#39;) . number of elements: 4 mean: 0.2535656690597534 and std: 0.1280052810907364 . torch.nn.init.kaiming_normal_(conv.weight,nonlinearity=&#39;relu&#39;); print(f&#39;mean: {conv.weight.mean()} and std: {conv.weight.std()} &#39;) . mean: 0.0030811727046966553 and std: 0.7828429937362671 . Don&#39;t worry seeing a high mean above, it&#39;s because our sample size is small as our filter only has 4 elements. In a normal convolution filter, this wouldn&#39;t be the case . Play around with your own network . deeplearning.ai has built an amazing interactive tool to test out all the things we have discussed above. Go check it out at https://www.deeplearning.ai/ai-notes/initialization/ . Okay, now why can&#39;t we trust PyTorch to initialize our weights for us by default? . I&#39;ve recently discovered that PyTorch does not use modern/recommended weight initialization techniques by default when creating Conv/Linear Layers. They&#39;ve been doing it using the old strategies so as to maintain backward compatibility in their code. I know it sounds strange, weird and very stupid but unfortunately it&#39;s true. As of 6th January 2022, this issue is still open on Github . Bug . torch.nn.modules.conv._ConvNd.reset_parameters?? . Signature: torch.nn.modules.conv._ConvNd.reset_parameters(self) Docstring: &lt;no docstring&gt; Source: def reset_parameters(self): init.kaiming_uniform_(self.weight, a=math.sqrt(5)) if self.bias is not None: fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight) bound = 1 / math.sqrt(fan_in) init.uniform_(self.bias, -bound, bound) File: ~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/conv.py Type: function . The above bug exists because PyTorch was adapted from Torch library, and authors found sqrt(5) to work well, but there&#39;s no justification or intuition behind this. Surprisingly, Tensorflow also uses the Xavier uniform initialization for Conv2d by default as well, which is again suboptimal when working with ReLU. . However, when PyTorch provides pretrained resnet and other architecture models, they cover up for this by explicitly initializing layers in the code with kaiming normal. You can see an example here. So this means . If you&#39;re importing a network from torchvision, it was initialized properly and there is nothing to worry about but | If you cut some of its layers and replace it with your own layers, you need to initialize them again as per recommended methods which is kaiming normal if you&#39;re working with ReLU function. | If you&#39;re writing your own layers which is what happens 99% of the case, initialize them explicitly | Solution . The most foolproof thing to do is to explicitly initialize the weights of your network using torch.nn.init . def conv(ni, nf, ks=3, stride=1, padding=1, **kwargs): _conv = nn.Conv2d(ni, nf, kernel_size=ks,stride=stride,padding=padding, **kwargs) nn.init.kaiming_normal_(_conv.weight) return _conv . Weight Initialization: Residual Networks . For Residual networks, the following code is used which initializes all Conv layers with Kaiming intialization, and BatchNorm layers with unit $ gamma$ and zero $ beta$ so that in the intial forward passes they act as identity function and do not affect the means and variance of activations. You can follow along the full code here . for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode=&#39;fan_out&#39;, nonlinearity=&#39;relu&#39;) elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) . Also, the second BatchNorm layer used in BasicBlock and Bottleneck of ResNet architecture is initialized to zero, for the reason of exploding activations explained below . # Zero-initialize the last BN in each residual branch, # so that the residual branch starts with zeros, and each residual block behaves like an identity. # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677 if zero_init_residual: for m in self.modules(): if isinstance(m, Bottleneck): nn.init.constant_(m.bn3.weight, 0) elif isinstance(m, BasicBlock): nn.init.constant_(m.bn2.weight, 0) . You can also do it recursively if you want . def init_cnn(m): if getattr(m, &#39;bias&#39;, None) is not None: nn.init.constant_(m.bias, 0) if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight) for l in m.children(): init_cnn(l) . Fixup Init . Training deep neural networks efficiently was a big problem in the deep learning community for a long time, that is until the BatchNorm paper came along. BatchNorm helps in stabilizing the layer activations and allows much deeper models to train without the problem of vanishing gradients. . But recently, a new paper called Fixup has shown that it&#39;s possible to train a network as deep as 100 layers without using BatchNorm, and instead using an appropriate initialization scheme for different types of layers. . . Problem : If we initialize with Kaiming: then $Var(F(x)) = Var(x)$. But now $Var(F(x) + x)$ will be greater than $Var(x)$ as variance grows with each block! . Solution: Initialize first conv with Kaiming, initialize second conv to zero. Then $Var(x + F(x)) = Var(x)$ . Intuition . I had a hard time getting my head around why initializing a layer with all zeros was not seen as a red flag here. That was exactly the first thing I had asked you to be cautious of. So what is going on here? I asked these questions on the fast.ai forum and Twitter and when I didn&#39;t get any replies :( I realized sometimes you need to answer your questions. . . Initial Confusion that I had . If last conv layer in a residual branch is initialized to zero, then there would be no symmetry breaking amongst different filters. All the filters learned in that layer would be identical, as each filter will . receive the same upstream gradient | will go over the same input/activation from previous layer and hence have the same local gradient | . Where was I wrong? . This blog post really helped me gain clarity on backpropagation in CNNs and was super helpful in understanding Fixup init. Another cool thing I learnt is that backpropagation in a CNN is also a convolution operation. Go through the post to go through the math . I was wrong to assume that each filter will receive the same upstream gradient. So in fact, each filter will receive different upstream gradient. | Each filter of a kernel is like an independent linear linear layer that outputs one channel of activations by going over the input. A filter will only receive upstream gradients from that single channel activations that it outputs. . All the individual activations outputted from each filter are combined to get the output size equal to the number of filters used in the kernel. . So can we use zero initialization in general? . No, it only works here because of the residual connection that exists right after the zero initialized Conv layer. The problem with zero initialized layer is that since its output will be zero, it will halt the gradient flow at the next layer by making the local gradients of weights for the next layer zero. . But in this case, the output will not be zero as a residual connnection exists and the activations from some previous layer is being added to the zero output before being processed by the next layer. . Papers Discussed . Understanding the difficulty of training deep feedforward neural networks by Glorot and Bengio | Delving Deep into Rectifiers by He et al. | Batch Normalization by Sergey Ioffe and Christian Szegedy | Fixup Initialization: Residual Learning Without Normalization by Zhang et al. | Extra Readings . If you want to get deep (pun intended) into playing around with these concepts I would highly recommend you to watch fast.ai&#39;s Deep Learning from the Foundations, spending most of your time reimplementing the notebooks by yourself. . 3Blue1Brown Neural Networks Playlist: If you&#39;re new to all this and confused about what networks have to do with linear algebra | Slides by Justin Johnson and CS231n The slides that inspired the above experiments | Yes you should understand backprop by Andrej Karpathy | https://pouannes.github.io/blog/initialization/ to read more theoretical proofs of Xavier and Kaiming init. | https://jimypbr.github.io/2020/02/fast-ai-lesson-8-notes-backprop-from-the-foundations | https://medium.com/comet-ml/selecting-the-right-weight-initialization-for-your-deep-neural-network-780e20671b22 |",
            "url": "https://adityassrana.github.io/blog/theory/2020/08/26/Weight-Init.html",
            "relUrl": "/theory/2020/08/26/Weight-Init.html",
            "date": " • Aug 26, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Vectorizing Nearest Neighbours Algorithm",
            "content": "This problem is similar to Stanford&#39;s kNN classifier assignment which implements this algorithm on the CIFAR-10 dataset . . Situation . In a hypothetical $n$-dimensional universe, there exists $p$ population of a particular species of humans, Homo BITSians. This species likes to hangout in specialized eateries, called Redis. In this universe, there are $q$ Redis which serve delicious snacks and beverages at nominal prices. Our task is to find the nearest Redi from each of the Homo BITSians so that they spend less time on commuting. . Problem . Matrices, $X in p times n$ and $Y in q times n$, which have the co-ordinates of $p$ Homo BITSians and $q$ Redis respectively in the $n$-dimensional universe are given. The $i^{th}$ row in the matrix, $X$, corresponds to the $i^{th}$ Homo BITSian. Similarly, the $i^{th}$ row in the matrix, $Y$, corresponds to the $i^{th}$ Redi. . Note: Here, row numbering (indexing) starts from $0$. . Task . Given $X$, $Y$, find a vector, $V$, of length $p$. The vector, $V$, is such that the $i^{th}$ element of $V$ has the index of the nearest Redi from the $i^{th}$ Homo BITSian. . Distance metric is the usual $l_2$-norm. In a n-dimensional space with points $x = (x_0, x_0, ldots, x_{n-1})$ and $y = (y_0, y_0, ldots, y_{n-1})$, the distance can be calculated as: . $$D_{xy}^2 = (x_0 - y_0)^2 + (x_1 - y_1)^2 + ldots + (x_{n-1} - y_{n-1})^2$$ Part 1: Find the index of the nearest Redi from each Homo BITSian . # Base Distance Function to be completed by the student import numpy as np def distances(X, Y): &quot;&quot;&quot; Given matrices X and Y, the function returns a distance matrix. The (i,j)th element of the matrix contains the distance of jth Redi from the ith Homo BITSian. Parameters: X,Y Returns: D &quot;&quot;&quot; ### BEGIN SOLUTION ### END SOLUTION return D . def nearest_redi(X, Y): &quot;&quot;&quot; Given matrices X and Y, the function returns a nearest redi vector. The i-th element of the vector contains the index of nearest Redi from the ith Homo BITSian. Parameters: X,Y Returns: V &quot;&quot;&quot; D = distances(X,Y) ### BEGIN SOLUTION ### END SOLUTION return V . Solutions begin from here . The way to understand the axis argument in numpy functions is that it collapses the specified axis. So when we specify the axis 1 (the column), it applies the function across all columns, resulting in a single column.For more intuition check out this post by Aerin Kim. In fact, I would reccommend you to read all of her posts. . def nearest_redi(X, Y): D = distances(X, Y) V = np.argmin(D, 1) return V . Let&#39;s evaluate the time taken by different approaches . X = np.random.randn(100,1024) Y = np.random.randn(1000,1024) . single loop using matrix addition/subtraction broadcast . def distances(X, Y): dist = np.zeros((X.shape[0], Y.shape[0])) for i,x in enumerate(X): dist[i] = ((X[i] - Y)**2).sum(axis=1) return dist %time test1 = nearest_redi(X, Y) . CPU times: user 166 ms, sys: 92 µs, total: 166 ms Wall time: 165 ms . no loops, using a gigantic outer product by forming a 3-d matrix of distances. Neat but still inefficient. Explained here . def distances(X, Y): x = X.reshape(X.shape[0], 1, X.shape[1]) dist = ((x - Y)**2).sum(axis = 2) return dist %time test2 = nearest_redi(X, Y) . CPU times: user 295 ms, sys: 96.3 ms, total: 392 ms Wall time: 390 ms . no loop, but breaking up the L2 norm into individual terms . $$ left | mathbf{I_1} - mathbf{I_2} right | = sqrt{ left | mathbf{I_1} right |^2 + left | mathbf{I_2} right | ^2 - 2 mathbf{I_1} cdot mathbf{I_2}} $$ . def distances(X, Y): dist = np.zeros((X.shape[0], Y.shape[0])) dist = (X**2).sum(axis=1)[:, np.newaxis] + (Y**2).sum(axis=1) - 2 * X.dot(Y.T) return dist %time test3 = nearest_redi(X, Y) . CPU times: user 12.8 ms, sys: 888 µs, total: 13.7 ms Wall time: 4.58 ms . Test Your Algorithm . #collapse-hide print(&quot;Running base test case 1...&quot;) X_test1 = np.array([[-3., 4.], [ 4., -2.], [-1., 0.]]) Y_test1 = np.array([[-3., 0.], [-3., -3.]]) V_test1 = nearest_redi(X_test1, Y_test1) V_ans_test1 = np.array([0, 1, 0]) assert np.array_equal(V_test1, V_ans_test1) print(&quot;Base test case 1 successful!! n&quot;) print(&quot;Running base test case 2...&quot;) X_test2 = np.array([[ 0.08170274, -4.8955951 , -4.0473417 ], [-1.13259313, 4.38171415, -3.22068891]]) Y_test2 = np.array([[ 3.79010736, 1.70042849, -3.06603884], [ 3.8921235 , -1.85207272, 2.33340715], [ 1.67360485, 2.11437547, 0.87529999]]) V_test2 = nearest_redi(X_test2, Y_test2) V_ans_test2 = np.array([0, 2]) assert np.array_equal(V_test2, V_ans_test2) print(&quot;Base test case 2 successful!! n&quot;) . . Running base test case 1... Base test case 1 successful!! Running base test case 2... Base test case 2 successful!! . #collapse-hide # Running hidden test case for Part 1. Don&#39;t edit the cell. *** 5 marks *** ### BEGIN HIDDEN TESTS X = np.array([[ 0.27170746, 0.89441607, 0.64849028], [ 0.42296173, 0.54342876, 0.47889235], [ 0.48688657, 0.11082849, 0.10691689], [ 0.04419385, 0.68777309, 0.49437059], [ 0.70143641, 0.09964604, 0.20949214], [ 0.01725016, 0.37424641, 0.94070338]]) Y = np.array([[ 0.24232741, 0.08413896, 0.014919 ], [ 0.15801316, 0.31713579, 0.0416702 ], [ 0.15784176, 0.50998073, 0.45405793], [ 0.44382259, 0.44515729, 0.49186482], [ 0.00695024, 0.23603969, 0.77601819]]) V = nearest_redi(X,Y) V_ans = np.array([2, 3, 0, 2, 0, 4]) assert np.array_equal(V, V_ans) ### END HIDDEN TESTS . . Extra Resources . https://www.labri.fr/perso/nrougier/from-python-to-numpy/ - Read the sections on Code Vectorization and Problem Vectorization | https://medium.com/@mikeliao/numpy-vectorization-d4adea4fc2a | https://mlxai.github.io/2017/01/03/finding-distances-between-data-points-with-numpy.html |",
            "url": "https://adityassrana.github.io/blog/broadcasting/numpy/2020/07/24/Vectorization.html",
            "relUrl": "/broadcasting/numpy/2020/07/24/Vectorization.html",
            "date": " • Jul 24, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "PyTorch Playground",
            "content": ". Dataset and Transforms . Dataset Class : manages the data, labels and data augmentations | DataLoader Class : manages the size of the minibatch | . Creating your Own Dataset . Let&#39;s take the example of training an autoencoder in which our training data only consists of images. . . The encoder can be made up of convolutional or linear layers. . . To create our own dataset class in PyTorch we inherit from the torch.utils.data.Dataset class and define two main methods, the __len__ and the __getitem__ . from torch.utils.data import Dataset from torchvision import transforms from PIL import Image from typing import List class ImageDataset(Dataset): &quot;&quot;&quot; A class for creating data and augemntation pipeline &quot;&quot;&quot; def __init__(self, glob_pattern:str, patchsize:int): &quot;&quot;&quot; Parameters - glob_pattern: this pattern must expand to a list of RGB images in PNG format. For eg. &quot;/data/train/cat/*.png&quot; patchsize: the size you want to crop the image to &quot;&quot;&quot; self.image_paths_list = glob.glob(glob_pattern) self.patchsize = patchsize def __len__(self): # denotes size of data return len(self.image_paths_list) def transform(self, image): # convert to RGB if image is B/W if image.mode == &#39;L&#39;: image = image.convert(&#39;RGB&#39;) self.data_transforms = transforms.Compose([transforms.RandomCrop(size = self.patchsize), transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(), transforms.ToTensor()]) return self.data_transforms(image) def __getitem__(self, index): # generates one sample of data image = Image.open(self.image_paths[index]) image= self.transform(image) return image . Transforms . Image processing operations using torchvision.transforms like cropping and resizing are done on the PIL Images and then they are converted to Tensors. The last transform which is transforms.ToTensor() seperates the the PIL Image into 3 channels (R,G,B) and scales its elements to the range (0,1). . A transform one observes a lot in Computer Vision based data pipelines is data normalization. . transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]) . If you&#39;re wondering where do these mean and std values come from, the answer is, the ImageNet dataset. It&#39;s a huge dataset of 14 million images and most pre-trained models are originally trained on this. The above values are the channel-wise mean and std of all the images in the dataset. So whenever you import a pre-trained model from torchvision, make sure you apply the normalization based on the statistics of the dataset that the model was trained on. Hence, the pipeline can be summarized as . Image --&gt; Crop/Resize --&gt; ToTensor --&gt; Normalize . To read more about why we normalize our data, read my blog post on this here . Tranforms functional API . The functional API is stateless and you can directly pass all the necessary arguments. Whereas torchvision.transforms are classes initialized with some default parameters unless specified. . # Class-based. Define once and use multiple times transform = transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) data = transform(data) # Functional. Pass parameters each time data = TF.normalize(data, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) . The functional API is very useful when transforming your data and target with the same random values, e.g. random cropping . import torchvision.transforms.functional as TF #it&#39;s not tensorflow :p i, j, h, w = transforms.RandomCrop.get_params(image, output_size=(512, 512)) image = TF.crop(image, i, j, h, w) mask = TF.crop(mask, i, j, h, w) . It also allows us to perform identical transforms on both image and target . def transform(self, image, mask): # Resize resize = transforms.Resize(size=(520, 520)) image = resize(image) mask = resize(mask # Random horizontal flipping if random.random() &gt; 0.5: image = TF.hflip(image) mask = TF.hflip(mask) # Random vertical flipping if random.random() &gt; 0.5: image = TF.vflip(image) mask = TF.vflip(mask) . DataLoaders . The data is passed to the model few samples at a time as datasets are usually too big to fit entirely on the CPU/GPU. . For choosing an appropriate batch_size, make it as high as possible as long as you dont encounter RuntimeError: CUDA out of memory and as long as it&#39;s a multiple of 16. . from torch.utils.data import DataLoader train_loader = DataLoader(train_dataset, batch_size = 32, shuffle=True, num_workers = 4) . Data Augmentation, where does it happen? . A lot of people get confused about how data augmentation helps in increasing the size of the dataset when we&#39;re not actually creating or saving new images. The point to understand here is that data augmentation happens on the fly. Every time __getitem__ method in the Dataset Class is called by the DataLoader, the transformations are applied. . When you use the dataloader in your training loop, at the start of every epoch it supplies a new data-augemnted dataset with the augmentations applied to each element. This means at each epoch, the model will see a new variant of the dataset. . for epoch in range(epochs): for data in train_loader(): . Kornia . Another thing to note is that these operations are performed on the CPU so you need to make sure that your data processing does not become your training bottleneck when using large batchsizes. This is the time for introducing - . . Kornia is a differentiable computer vision library for PyTorch that operates directly on tensors, hence letting you make full use of your GPUs. . Writing Custom Autograd Functions / Layers . Writing your own ReLU . class MyReLU(torch.autograd.Function): @staticmethod def forward(ctx, i): &quot;&quot;&quot; ctx is a context object that can be used to stash information for backward computation. You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method. &quot;&quot;&quot; input = i.clone() ctx.save_for_backward(input) return input.clamp(min=0) @staticmethod def backward(ctx, grad_output): &quot;&quot;&quot; In the backward pass we receive a Tensor containing the gradient of the loss wrt the output, and we need to compute the gradient of the loss wrt the input. &quot;&quot;&quot; input, = ctx.saved_tensors grad_input = grad_output.clone() grad_input[input &lt; 0] = 0 return grad_input . Understanding Gradient Shape . . Gradient returned by the backward method of the class should have the same shape as the input to the forward method of the class, so that the gradient computed for the input after the loss.backward() step has the same shape as input and can be used to update it in the optimizer.step() . loss.backward() computes d(loss)/d(w) for every parameter which has requires_grad=True. They are accumulated in w.grad. And the optimizer.step() updates w using w.grad, w += -lr* x.grad . For more info read the posts below . PyTorch Custom Layers | PyTorch Source Code Examples on Github | PyTorch official docs | . Avoid using in-place operations as they cause problems while back-propagation because of the way they modify the graph. As a precaution, always clone the input in the forward pass, and clone the incoming gradients before modifying them. . An in-place operation directly modifies the content of a given Tensor without making a copy. Inplace operations in PyTorch are always postfixed with a , like .add() or .scatter_(). Python operations like + = or *= are also in-place operations. . Dealing with non-differentiable functions . Sometimes in your model or loss calculation you need to use functions that are non-differentiable. For calculating gradients, autograd requires all components of the graph to be differentiable. You can work around this by using a proxy function in the backward pass calculations. . f_hard : non-differentiable f_soft : differentiable proxy for w_hard . f_out = f_soft + (f_hard - f_soft).detach() # in PyTorch f_out = f_soft + tf.stop_grad(f_hard - f_soft) # in Tensorflow . Core Idea . y = x_backward + (x_forward - x_backward).detach() . It gets you x_forward in the forward pass, but derivative acts as if you had x_backward . Example . class Binarizer(torch.autograd.Function): &quot;&quot;&quot; An elementwise function that bins values to 0 or 1 depending on a threshold of 0.5, but in backward pass acts as an identity layer. Such layers are also known as straight-through gradient estimators Input: a tensor with values in range (0,1) Returns: a tensor with binary values: 0 or 1 based on a threshold of 0.5 Equation(1) in paper &quot;&quot;&quot; @staticmethod def forward(ctx, i): return (i&gt;0.5).float() @staticmethod def backward(ctx, grad_output): return grad_output def bin_values(x): return Binarizer.apply(x) . The above function can be reimplemented with a single line in Pytorch while maintaining differentiabilty . def bin_values(x): return x + ((x&gt;0.5).float() - x).detach() . Basic Training and Validation Loop . def fit(epochs, model, loss_func, opt, train_dl, valid_dl): for epoch in range(epochs): # Handle batchnorm / dropout model.train() # print(model.training) for mini_batch in train_dl: pred = model(mini_batch) loss = loss_func(pred, target) loss.backward() optimizer.step() optimizer.zero_grad() model.eval() #print(model.training) with torch.no_grad(): for mini_batch in valid_dl: pred = model(mini_batch) # log some metrics here # aggregate metrics from all batches . Once you become more familiar with writing training and validation loops, I would recommend you to try out PyTorch Lightning PyTorch Lightning , which is a great library started by William Falcon that helps you get rid of all the PyTorch boilerplate code and instead lets you focus on the research part of your project. . Tensorboard . Installing . Install tensorboard with pip install tensorboard . . Creating a SummaryWriter . from torch.utils.tensorboard import SummaryWriter writer_train = SummaryWriter(os.path.join(args.experiment_dir,&quot;tensorboard&quot;)) . Scalars . Logging statements are added at different steps in the training loop wherever you want to log something. You can track scalars, images and even histograms. You can read more about this on the official PyTorch docs . Logging scalars can be as simple as . writer_train.add_scalar(&#39;train_loss&#39;, loss.item(), iteration) . where iteration is the global_step_count that you can keep track of inside your training loop. . Images . We&#39;ll use make_grid to create a grid of images directly from tensors so that we can plot them together. . from torchvision.utils import make_grid # x is a tensor of Images of the shape (N,3,H,W) x_grid = make_grid(x[:5],nrow=5) writer_train.add_image(&#39;train/original_images&#39;,x_grid, iteration) . Launch . To visualize what you&#39;ve logged, launch a tensorboard instance from the terminal by entering tensorboard --logdir . in the directory where you have logged your experiments. . Inference . To make predictions out of your trained model, make sure you feed data in the right format. . Input Tensor Format : (batch_size, channels, height, width). The model and the convolutional layers expect the input tensor to be of the shape (N,C,H,W), so when feeding an image/images to the model, add a dimension for batching. . Converting from img--&gt;numpy representation and feeding the model gives an error because the input is in ByteTensor format. Only float operations are supported for conv-like operations. So add an extra step after numpy conversion - . img = img.type(&#39;torch.DoubleTensor&#39;) . Saving and Loading Models . PyTorch saves a model as a state_dict and the extension used is .pt . torch.save(model.state_dict(), PATH = &#39;latest_checkpoint.pt&#39;) . Sometimes you add new layers to your model which which were not present in the model you saved as checkpoint. In such a case set the strict keyword to False . model = Model() checkpoint = torch.load(&#39;latest_checkpoint.pt&#39;) model.load_state_dict(checkpoint, strict=False) . On Loading a model, if it shows a message like this, it means there were no missing keys and everything went well ( it&#39;s not an error ). . IncompatibleKeys(missing_keys=[], unexpected_keys=[]) . Keyboard interrupt and saving the last state of a model if you need to stop the experiment mid-way of training: . try: # training code here except KeyboardInterrupt: # save model here . Extra Resources . Grokking PyTorch | Effective PyTorch | The Python Magic Behind PyTorch | Python is Cool - ChipHuyen | PyTorch StyleGuide | Clean Code Python | Using _ in Variable Naming | Pytorch Coding Conventions | Fine Tuning etc | https://github.com/dsgiitr/d2l-pytorch | https://github.com/L1aoXingyu/pytorch-beginner | https://github.com/yunjey/pytorch-tutorial | https://github.com/MorvanZhou/PyTorch-Tutorial | .",
            "url": "https://adityassrana.github.io/blog/tutorials/2020/04/22/PyTorch-Playground.html",
            "relUrl": "/tutorials/2020/04/22/PyTorch-Playground.html",
            "date": " • Apr 22, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://adityassrana.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Resources",
          "content": "Coming Soon .",
          "url": "https://adityassrana.github.io/blog/resources/",
          "relUrl": "/resources/",
          "date": ""
      }
      
  

  

  

  

  
  

  
  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://adityassrana.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}