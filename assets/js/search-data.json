{
  
    
        "post0": {
            "title": "Don't Trust PyTorch to Initialize Your Variables",
            "content": "This post is a summary of things I learnt while doing Fast.ai&#39;s Deep Learning from Foundations and Justin Johnsons&#39;s Computer Vision Course at UMichigan which is the updated and latest version of CS231n. . Situation . You want to solve a problem using deep learning. You have collected a dataset, decided on a neural network architecture, loss function, optimizer and some metrics you want to evaluate on. While training you notice your network isn&#39;t performing well, neither on train nor validation dataset. Looking for bugs while training neural networks is not a simple task, so we break down the whole training process into separate pipelines. Let&#39;s start by looking for bugs in our architecture and the way we initialize our weights. . Problem: Why does good initialization matter? . As Neural Networks involve a lot of matrix multiplications, the mean and variance of activations can quickly shoot off to very high values or drop down to zero. This will cause the local gradients of our layers to become NaN or zero and hence prevent our network from learning anything . A common strategy to avoid this is to initialize the weights of your network using the latest techniques. For example if you’re using ReLU activation after a layer, you must initialize your weights with Kaiming He initialization and set the biases to zero.(This was introduced in the 2014 ImageNet winning paper from Microsoft). This ensures the mean and standard deviation of activations of all layers stay close to 0 and 1 respectively. . . Upstream gradients are multiplied by local gradients to get the downstream gradients during backpropLet&#39;s compare the local gradient behavior of some common activation functions . . Notice how the gradients in both sigmoid and tanh are non-zero only inside a certain range between [-5, 5] . Also notice that when using sigmoid, the local gradient achieves a maximum value of 0.25, thus every time gradient passes through a sigmoid layer, it gets diminished by at least 75%. . Task . To ensure the gradients of our network do not explode or diminish to zero . The Mean of activations should be zero | The Variance of activations should stay same across layers. | Intitalizing the network with the right weights can make the difference between converging in a few epochs versus not converging at all. . Solution: Let&#39;s Compare Differrent Initialization Strategies . You must be wondering that surely it cannot be that bad. Let&#39;s consider a forward pass for a 6-layer neural network with each hidden layer the size of 4096 and tanh activation and let&#39;s plot the histogram for activations after each layer . All Weights and Biases Set to Zero . . Image SourceDo not ever do this! In such a case, all neurons of a layer would end up computing the same output, will receive the same gradients during backpropagation and undergo the same parameter updates. That means that all neurons of a layer will be learning the same features during training as there is no symmetry breaking. Also The problem with zero initialized layer is that since its output will be zero, it will halt the gradient flow at the next layer by making the local gradients of weights for the next layer zero. . import numpy as np import matplotlib.pyplot as plt %matplotlib inline . plt.style.use(&#39;seaborn&#39;) . Using Small Random Numbers from a Normal Distribution . Why Normal/Gaussian? Because it is characterized by its mean and variance, exactly the two things we want to control and compare . dims = [4096]*7 mean = [] std = [] hist = [] x = np.random.randn(dims[0]) for Din, Dout in zip(dims[:-1],dims[1:]): #Focus here W = 0.01*np.random.randn(Din,Dout) x = np.tanh(x.dot(W)) mean.append(x.mean()) std.append(x.std()) hist.append(x) fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 8)) for i,ax,m,s in zip(np.arange(6),axes.flatten(),mean,std): ax.hist(hist[i],density = True,bins=50) ax.set_xlim(-1.2,1.2) ax.set_title(f&#39;Layer:{i+1} mean:{m:.2f} and std;{s:.2f}&#39;) plt.tight_layout() . Why and when do gradients vanish? . As all activations tend to zero for deeper layers, the gradients are going to collapse to zero as well.Why? . . Because whenever we compute local gradient on the weight, the local gradient of the weight is going to be equal to to the activation of the previous layer. So this means for a deep network, if the activations before a layer collapse to zero, the local gradients on the weight collapse as well and the network will stop learning. That&#39;s why for gradients to stop vanishing or exploding it&#39;s important to prevent activations from vanishing or exploding respectively. . . Backprop for a Linear Layer . If this part is unclear to you, you should revise backpropagation for a linear layer by having a look at this post by Justin Johnson. For extra resources you can also have a look at . http://cs231n.stanford.edu/handouts/linear-backprop.pdf | http://cs231n.stanford.edu/handouts/derivatives.pdf | https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf | https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture5.pdf | fig, ax = plt.subplots(figsize=(3,3)) ax.hist(hist[5],bins=50) ax.set_title(f&#39;Layer:{6} mean:{m:.2f} and std;{s:.2f}&#39;) ax.set_xlim(-1,1) plt.tight_layout() . As we can see, most of the activations in the last layer are around zero. . The same activations as above super-imposed on each other. Plotting this just because it seems visually appealing to me . plt.style.use(&#39;default&#39;) for m,s in zip(mean,std): a = np.random.normal(m,s,size=1000) plt.hist(a,bins=50) . plt.style.use(&#39;seaborn&#39;) . Maybe larger weights will not get diminished . dims = [4096]*7 mean = [] std = [] hist = [] x = np.random.randn(dims[0]) for Din, Dout in zip(dims[:-1],dims[1:]): W = 0.5*np.random.randn(Din,Dout) x = np.tanh(x.dot(W)) mean.append(x.mean()) std.append(x.std()) hist.append(x) fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 8)) for i,ax,m,s in zip(np.arange(6),axes.flatten(),mean,std): ax.hist(hist[i],density = True,bins=50) ax.set_xlim(-1.2,1.2) ax.set_title(f&#39;Layer:{i+1} mean:{m:.2f} and std;{s:.2f}&#39;) plt.tight_layout() . Wow, that was a disaster. All neurons are saturated and are outputting -1s and 1s where gradient is zero. Again, no learning . Xavier : Magic Scaling Number $$ sqrt{ frac{1}{Din}}$$ . dims = [4096]*7 mean = [] std = [] hist = [] x = np.random.randn(dims[0]) for Din, Dout in zip(dims[:-1],dims[1:]): W = np.random.randn(Din,Dout)/np.sqrt(Din) x = np.tanh(x.dot(W)) mean.append(x.mean()) std.append(x.std()) hist.append(x) fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 8)) for i,ax,m,s in zip(np.arange(6),axes.flatten(),mean,std): ax.hist(hist[i],density = True,bins=50) ax.set_xlim(-1.2,1.2) ax.set_title(f&#39;Layer:{i+1} mean:{m:.2f} and std;{s:.2f}&#39;) plt.tight_layout() . Hmmmmmmmmm, why did that work? This was proposed in Understanding the difficulty of training deep feedforward neural networks, the 2010 paper that introduced ‘Xavier initialization’ which we have just used. . But where does it come from? It&#39;s not that mysterious if you remember the definition of the matrix multiplication. When we do y = x @ W, the coefficients of y are defined by . $$y_{i} = x_{1} w_{1} + x_{2} w_{2} + cdots + x_{n} w_{n} = sum_{j=1}^{Din} x_{j} w_{j} $$ . Now at the very beginning, our x vector has a mean of roughly 0. and a standard deviation of roughly 1. (since we picked it that way). . $$ Var(y_{i}) = Var(x_{1} w_{1} + x_{2} w_{2} + cdots + x_{n} w_{n}) $$ . Assume x, w are iid $$ Var(y_{i}) = D_{in} * Var(x_{i} w_{i}) $$ | Assume x, w independent $$ Var(y_{i}) = D_{in} * (E[x_{i}^2] E[w_{i}^2] - E[x_{i}]^2 E[w_{i}]^2) $$ | Assume x, w are zero-mean $$ Var(y_{i}) = D_{in} * Var(x_{i}) * Var(w_{i}) $$ | Hence for the variances to remain same between layers that is $$ Var(y_{i}) = Var(x_{i})$$ the condition is $$ Var(w_i) = frac{1}{Din} $$ . That&#39;s it, the Xavier initialization. . Kaiming Initialization, if using ReLU scale by $$ sqrt{ frac{2}{Din}}$$ . Xavier initialization assumes the input to have zero mean, but things change when we use a ReLU which sets all negative values to zero. Let&#39;s see what happens if we continue using Xavier initialization with ReLU . dims = [4096]*7 mean = [] std = [] hist = [] x = np.random.randn(dims[0]) for Din, Dout in zip(dims[:-1],dims[1:]): W = np.random.randn(Din,Dout)/np.sqrt(Din) x = np.maximum(0, x.dot(W)) print(x.shape) mean.append(x.mean()) std.append(x.std()) hist.append(x) fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 8)) for i,ax,m,s in zip(np.arange(6),axes.flatten(),mean,std): ax.hist(hist[i],density = True,bins=50) ax.set_xlim(-1.2,1.5) ax.set_title(f&#39;Layer:{i+1} mean:{m:.2f} and std;{s:.2f}&#39;) plt.tight_layout() . (4096,) (4096,) (4096,) (4096,) (4096,) (4096,) . Things aren&#39;t looking too good as most activations are again driven to zero on reaching the last layer. Let&#39;s try out Kaiming init now . dims = [4096]*7 mean = [] std = [] hist = [] x = np.random.randn(dims[0]) for Din, Dout in zip(dims[:-1],dims[1:]): W = np.random.randn(Din,Dout)/np.sqrt(Din/2) x = np.maximum(0, x.dot(W)) mean.append(x.mean()) std.append(x.std()) hist.append(x) fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 8)) for i,ax,m,s in zip(np.arange(6),axes.flatten(),mean,std): ax.hist(hist[i],density = True,bins=50) ax.set_xlim(-1.2,1.5) ax.set_title(f&#39;Layer:{i+1} mean:{m:.2f} and std;{s:.2f}&#39;) plt.tight_layout() . The activations look much better now . Here&#39;s a sample code if you get confused how to add multiple plots to the same figure. You can read more about this here . #collapse-hide xs = np.linspace(0, 2 * np.pi, 400) ys = np.sin(xs ** 2) xc = np.linspace(0, 2 * np.pi, 600) yc = np.cos(xc ** 2) fig, ax = plt.subplots(2, 2, figsize=(16, 8)) # `Fig` is short for Figure. `ax` is short for Axes. ax[0, 0].plot(xs, ys) ax[1, 1].plot(xs, ys) ax[0, 1].plot(xc, yc) ax[1, 0].plot(xc, yc) fig.suptitle(&quot;Basic plotting using Matplotlib&quot;) plt.show() . . How to calculate fan-in and fan-out in Xavier initialization for CNNs? . While reading the papers on initialization you&#39;ll come across these two terms &#39;fan-in&#39; and &#39;fan-out&#39; quite often. . This part of my post is heavily inspired by Matthew Kleinsmith&#39;s post on CNN Visualizations on Medium . . fan-in is the same as Din that we have used in our code above. Similarly a Conv Layer can be seen as a Linear layer. . . The Image . The Filter . Since the filter fits in the image four times, we have four results . Here’s how we applied the filter to each section of the image to yield each result . The equation view . The compact equation viewand now most importantly the neural network view where you can see each output is generated from 4 inputs and hence fan_in = 4. . . If the original image had been a 3-channel image, each output would be generated from 3*4 = 12 inputs and hence fan_in would be 12. Hence, . receptive_field_size = kernel_height * kernel_width fan_in = num_input_feature_maps * receptive_field_size fan_out = num_output_feature_maps * receptive_field_size . I would also encourage you to play around with the PyTorch functions for calculating fan_in and fan_out here.. Somewhat like this, referring to the example above . import torch conv = torch.nn.Conv2d(in_channels=1,out_channels=1,kernel_size=2) print(f&#39;Conv shape: {conv.weight.shape}&#39;) . Conv shape: torch.Size([1, 1, 2, 2]) . #returns fan_in and fan_out value for a layer torch.nn.init._calculate_fan_in_and_fan_out(conv.weight) . (4, 4) . print(f&#39;number of elements: {conv.weight.numel()}&#39;) print(f&#39;mean: {conv.weight.mean()} and std: {conv.weight.std()} &#39;) . number of elements: 4 mean: 0.04142443835735321 and std: 0.1813054084777832 . torch.nn.init.kaiming_normal_(conv.weight,nonlinearity=&#39;relu&#39;); print(f&#39;mean: {conv.weight.mean()} and std: {conv.weight.std()} &#39;) . mean: -0.09964150935411453 and std: 0.40762221813201904 . Don&#39;t worry seeing a high mean above, it&#39;s because our sample size is small as our filter only has 4 elements. In a normal convolution filter, this wouldn&#39;t be the case . Play around with your own network . deeplearning.ai has built an amazing interactive tool to test out all the things we have discussed above. Go check it out at https://www.deeplearning.ai/ai-notes/initialization/ . Okay, now why can&#39;t we trust PyTorch to initialize our weights for us by default? . I&#39;ve recently discovered that PyTorch does not use modern/recommended weight initialization techniques by default when creating Conv/Linear Layers. They&#39;ve been doing it using the old strategies so as to maintain backward compatibility in their code. I know it sounds strange, weird and very stupid but unfortunately it&#39;s true. As of 26th August 2020, this issue is still open on Github . Bug . torch.nn.modules.conv._ConvNd.reset_parameters?? . Signature: torch.nn.modules.conv._ConvNd.reset_parameters(self) Docstring: &lt;no docstring&gt; Source: def reset_parameters(self): init.kaiming_uniform_(self.weight, a=math.sqrt(5)) if self.bias is not None: fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight) bound = 1 / math.sqrt(fan_in) init.uniform_(self.bias, -bound, bound) File: ~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/conv.py Type: function . The above bug exists because PyTorch was adapted from Torch library, and authors found sqrt(5) to work well, but there&#39;s no justification or intuition behind this. Surprisingly, Tensorflow also uses the Xavier uniform initialization for Conv2d by default as well, which is again suboptimal when working with ReLU. . However, when PyTorch provides pretrained resnet and other architecture models, they cover up for this by explicitly initializing layers in the code with kaiming normal. You can see an example here. So this means . If you&#39;re importing a network from torchvision, it was initialized properly and there is nothing to worry about but | If you cut some of its layers and replace it with your own layers, you need to initialize them again as per recommended methods which is kaiming normal if you&#39;re working with ReLU function. | If you&#39;re writing your own layers which is what happens 99% of the case, initialize them explicitly | Solution . The most foolporoof thing to do is to explicitly initialize the weights of your network using torch.nn.init . def conv(ni, nf, ks=3, stride=1, padding=1, **kwargs): _conv = nn.Conv2d(ni, nf, kernel_size=ks,stride=stride,padding=padding, **kwargs) nn.init.kaiming_normal_(_conv.weight) return _conv . Weight Initialization: Residual Networks . For Residual networks, the following code is used which initializes all Conv layers with Kaiming intialization, and BatchNorm layers with unit $ gamma$ and zero $ beta$ so that in the intial forward passes they act as identity function and do not affect the means and variance of activations. You can follow along the full code here . for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode=&#39;fan_out&#39;, nonlinearity=&#39;relu&#39;) elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) . Also, the second BatchNorm layer used in BasicBlock and Bottleneck of ResNet architecture is initialized to zero, for the reason of exploding activations explained below . # Zero-initialize the last BN in each residual branch, # so that the residual branch starts with zeros, and each residual block behaves like an identity. # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677 if zero_init_residual: for m in self.modules(): if isinstance(m, Bottleneck): nn.init.constant_(m.bn3.weight, 0) elif isinstance(m, BasicBlock): nn.init.constant_(m.bn2.weight, 0) . You can also do it recursively if you want . def init_cnn(m): if getattr(m, &#39;bias&#39;, None) is not None: nn.init.constant_(m.bias, 0) if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight) for l in m.children(): init_cnn(l) . Fixup Init . Training deep neural networks efficiently was a big problem in the deep learning community for a long time, that is until the BatchNorm paper came along. BatchNorm helps in stabilizing the layer activations and allows much deeper models to train without the problem of vanishing gradients. . But recently, a new paper called Fixup has shown that it&#39;s possible to train a network as deep as 100 layers without using BatchNorm, and instead using an appropriate initialization scheme for different types of layers. . . Problem : If we initialize with Kaiming: then $Var(F(x)) = Var(x)$. But now $Var(F(x) + x)$ will be greater than $Var(x)$ as variance grows with each block! . Solution: Initialize first conv with Kaiming, initialize second conv to zero. Then $Var(x + F(x)) = Var(x)$ . Intuition . I had a hard time getting my head around why initializing a layer with all zeros was not seen as a red flag here. That was exactly the first thing I had asked you to be cautious of. So what is going on here? I asked these questions on the fast.ai forum and Twitter and when I didn&#39;t get any replies :( I realized sometimes you need to answer your questions. . . Initial Confusion that I had . If last conv layer in a residual branch is initialized to zero, then there would be no symmetry breaking amongst different filters. All the filters learned in that layer would be identical, as each filter will . receive the same upstream gradient | will go over the same input/activation from previous layer and hence have the same local gradient | . Where was I wrong? . This blog post really helped me gain clarity on backpropagation in CNNs and was super helpful in understanding Fixup init. Another cool thing I learnt is that backpropagation in a CNN is also a convolution operation. Go through the post to go through the math . I was wrong to assume that each filter will receive the same upstream gradient. So in fact, each filter will receive different upstream gradient. | Each filter of a kernel is like an independent linear linear layer that outputs one channel of activations by going over the input. A filter will only receive upstream gradients from that single channel activations that it outputs. . All the individual activations outputted from each filter are combined to get the output size equal to the number of filters used in the kernel. . So can we use zero initialization in general? . No, it only works here because of the residual connection that exists right after the zero initialized Conv layer. The problem with zero initialized layer is that since its output will be zero, it will halt the gradient flow at the next layer by making the local gradients of weights for the next layer zero. . But in this case, the output will not be zero as a residual connnection exists and the activations from some previous layer is being added to the zero output before being processed by the next layer. . Papers Discussed . Understanding the difficulty of training deep feedforward neural networks by Glorot and Bengio | Delving Deep into Rectifiers by He et al. | Batch Normalization by Sergey Ioffe and Christian Szegedy | Fixup Initialization: Residual Learning Without Normalization by Zhang et al. | Extra Readings . If you want to get deep (pun intended) into playing around with these concepts I would highly recommend you to watch fast.ai&#39;s Deep Learning from the Foundations, spending most of your time reimplementing the notebooks by yourself. . 3Blue1Brown Neural Networks Playlist: If you&#39;re new to all this and confused about what networks have to do with linear algebra | Slides by Justin Johnson and CS231n The slides that inspired the above experiments | Yes you should understand backprop by Andrej Karpathy | https://pouannes.github.io/blog/initialization/ to read more theoretical proofs of Xavier and Kaiming init. | https://jimypbr.github.io/2020/02/fast-ai-lesson-8-notes-backprop-from-the-foundations | https://medium.com/comet-ml/selecting-the-right-weight-initialization-for-your-deep-neural-network-780e20671b22 |",
            "url": "https://adityassrana.github.io/blog/theory/2020/08/26/Weight-Init.html",
            "relUrl": "/theory/2020/08/26/Weight-Init.html",
            "date": " • Aug 26, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Vectorizing Nearest Neighbours Algorithm",
            "content": "This problem is similar to Stanford&#39;s kNN classifier assignment which implements this algorithm on the CIFAR-10 dataset . . Situation . In a hypothetical $n$-dimensional universe, there exists $p$ population of a particular species of humans, Homo BITSians. This species likes to hangout in specialized eateries, called Redis. In this universe, there are $q$ Redis which serve delicious snacks and beverages at nominal prices. Our task is to find the nearest Redi from each of the Homo BITSians so that they spend less time on commuting. . Problem . Matrices, $X in p times n$ and $Y in q times n$, which have the co-ordinates of $p$ Homo BITSians and $q$ Redis respectively in the $n$-dimensional universe are given. The $i^{th}$ row in the matrix, $X$, corresponds to the $i^{th}$ Homo BITSian. Similarly, the $i^{th}$ row in the matrix, $Y$, corresponds to the $i^{th}$ Redi. . Note: Here, row numbering (indexing) starts from $0$. . Task . Given $X$, $Y$, find a vector, $V$, of length $p$. The vector, $V$, is such that the $i^{th}$ element of $V$ has the index of the nearest Redi from the $i^{th}$ Homo BITSian. . Distance metric is the usual $l_2$-norm. In a n-dimensional space with points $x = (x_0, x_0, ldots, x_{n-1})$ and $y = (y_0, y_0, ldots, y_{n-1})$, the distance can be calculated as: . $$D_{xy}^2 = (x_0 - y_0)^2 + (x_1 - y_1)^2 + ldots + (x_{n-1} - y_{n-1})^2$$ Part 1: Find the index of the nearest Redi from each Homo BITSian . # Base Distance Function to be completed by the student import numpy as np def distances(X, Y): &quot;&quot;&quot; Given matrices X and Y, the function returns a distance matrix. The (i,j)th element of the matrix contains the distance of jth Redi from the ith Homo BITSian. Parameters: X,Y Returns: D &quot;&quot;&quot; ### BEGIN SOLUTION ### END SOLUTION return D . def nearest_redi(X, Y): &quot;&quot;&quot; Given matrices X and Y, the function returns a nearest redi vector. The i-th element of the vector contains the index of nearest Redi from the ith Homo BITSian. Parameters: X,Y Returns: V &quot;&quot;&quot; D = distances(X,Y) ### BEGIN SOLUTION ### END SOLUTION return V . Solutions begin from here . The way to understand the axis argument in numpy functions is that it collapses the specified axis. So when we specify the axis 1 (the column), it applies the function across all columns, resulting in a single column.For more intuition check out this post by Aerin Kim. In fact, I would reccommend you to read all of her posts. . def nearest_redi(X, Y): D = distances(X, Y) V = np.argmin(D, 1) return V . Let&#39;s evaluate the time taken by different approaches . X = np.random.randn(100,1024) Y = np.random.randn(1000,1024) . single loop using matrix addition/subtraction broadcast . def distances(X, Y): dist = np.zeros((X.shape[0], Y.shape[0])) for i,x in enumerate(X): dist[i] = ((X[i] - Y)**2).sum(axis=1) return dist %time test1 = nearest_redi(X, Y) . CPU times: user 166 ms, sys: 92 µs, total: 166 ms Wall time: 165 ms . no loops, using a gigantic outer product by forming a 3-d matrix of distances. Neat but still inefficient. Explained here . def distances(X, Y): x = X.reshape(X.shape[0], 1, X.shape[1]) dist = ((x - Y)**2).sum(axis = 2) return dist %time test2 = nearest_redi(X, Y) . CPU times: user 295 ms, sys: 96.3 ms, total: 392 ms Wall time: 390 ms . no loop, but breaking up the L2 norm into individual terms . $$ left | mathbf{I_1} - mathbf{I_2} right | = sqrt{ left | mathbf{I_1} right |^2 + left | mathbf{I_2} right | ^2 - 2 mathbf{I_1} cdot mathbf{I_2}} $$ . def distances(X, Y): dist = np.zeros((X.shape[0], Y.shape[0])) dist = (X**2).sum(axis=1)[:, np.newaxis] + (Y**2).sum(axis=1) - 2 * X.dot(Y.T) return dist %time test3 = nearest_redi(X, Y) . CPU times: user 12.8 ms, sys: 888 µs, total: 13.7 ms Wall time: 4.58 ms . Test Your Algorithm . #collapse-hide print(&quot;Running base test case 1...&quot;) X_test1 = np.array([[-3., 4.], [ 4., -2.], [-1., 0.]]) Y_test1 = np.array([[-3., 0.], [-3., -3.]]) V_test1 = nearest_redi(X_test1, Y_test1) V_ans_test1 = np.array([0, 1, 0]) assert np.array_equal(V_test1, V_ans_test1) print(&quot;Base test case 1 successful!! n&quot;) print(&quot;Running base test case 2...&quot;) X_test2 = np.array([[ 0.08170274, -4.8955951 , -4.0473417 ], [-1.13259313, 4.38171415, -3.22068891]]) Y_test2 = np.array([[ 3.79010736, 1.70042849, -3.06603884], [ 3.8921235 , -1.85207272, 2.33340715], [ 1.67360485, 2.11437547, 0.87529999]]) V_test2 = nearest_redi(X_test2, Y_test2) V_ans_test2 = np.array([0, 2]) assert np.array_equal(V_test2, V_ans_test2) print(&quot;Base test case 2 successful!! n&quot;) . . Running base test case 1... Base test case 1 successful!! Running base test case 2... Base test case 2 successful!! . #collapse-hide # Running hidden test case for Part 1. Don&#39;t edit the cell. *** 5 marks *** ### BEGIN HIDDEN TESTS X = np.array([[ 0.27170746, 0.89441607, 0.64849028], [ 0.42296173, 0.54342876, 0.47889235], [ 0.48688657, 0.11082849, 0.10691689], [ 0.04419385, 0.68777309, 0.49437059], [ 0.70143641, 0.09964604, 0.20949214], [ 0.01725016, 0.37424641, 0.94070338]]) Y = np.array([[ 0.24232741, 0.08413896, 0.014919 ], [ 0.15801316, 0.31713579, 0.0416702 ], [ 0.15784176, 0.50998073, 0.45405793], [ 0.44382259, 0.44515729, 0.49186482], [ 0.00695024, 0.23603969, 0.77601819]]) V = nearest_redi(X,Y) V_ans = np.array([2, 3, 0, 2, 0, 4]) assert np.array_equal(V, V_ans) ### END HIDDEN TESTS . . Extra Resources . https://www.labri.fr/perso/nrougier/from-python-to-numpy/ - Read the sections on Code Vectorization and Problem Vectorization | https://medium.com/@mikeliao/numpy-vectorization-d4adea4fc2a | https://mlxai.github.io/2017/01/03/finding-distances-between-data-points-with-numpy.html |",
            "url": "https://adityassrana.github.io/blog/broadcasting/numpy/2020/07/24/Vectorization.html",
            "relUrl": "/broadcasting/numpy/2020/07/24/Vectorization.html",
            "date": " • Jul 24, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://adityassrana.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "PyTorch Tips and Tricks",
            "content": "PyTorch . Input Tensor Format : (N,C,H,W). The model and the convolutional layers expect the input tensor to be of this shape, so when feeding an image/images to the model, add a dimension for batching. . Converting from img–&gt;numpy representation and feeding the model gives an error because the input is in ByteTensor format. Only float operations are supported for conv-like operations. . img = img.type(&#39;torch.DoubleTensor&#39;) . Dataset and Transforms . Dataset Class : what data will be input to the model and what augmentations will be applied | DataLoader Class : how big a minibatch will be, | . To create our own dataset class in PyTorch we inherit from the Dataset Class and define two main methods, the __len__ and the __getitem__ . import torch from PIL import Image import torchvision import torchvision.transforms.functional as TF #it&#39;s not tensorflow class ImageDataset(torch.utils.data.Dataset): &quot;&quot;&quot;Dataset class for creating data pipeline for images&quot;&quot;&quot; def __init__(self, train_glob, patchsize): &quot;&quot;&quot;&quot; train_glob is a Glob pattern identifying training data. This pattern must expand to a list of RGB images in PNG format. for eg. &quot;/images/cat/*.png&quot; patchsize is the crop size you want from the image &quot;&quot;&quot; self.list_id = glob.glob(train_glob) self.patchsize = patchsize def __len__(self): #denotes total number of samples return len(self.list_id) def __getitem__(self, index): #generates one sample of data image = Image.open(self.list_id[index]) # convert to RGB if image is B/W if image.mode == &#39;L&#39;: image = image.convert(&#39;RGB&#39;) image= self.transform(image) return image def transform(self,image): # Fucntional transforms allow us to apply # the same crop on semantic segmentation i, j, h, w = torchvision.transforms.RandomCrop.get_params(image , output_size = (self.patchsize, self.patchsize)) image = TF.crop(image, i, j, h, w) image = TF.to_tensor(image) return image . Image processing operations like cropping and resizing should be done on the PIL Image and not the tensor . Image --&gt; Crop/Resize --&gt; toTensor --&gt; Normalize . The transforms.ToTensor() or TF.to_tensor (functional version of the same command) separates the PIL Image into 3 channels (R,G,B), converts it to the range (0,1). You can multiply by 255 to get the range (0,255). . Using transforms.Normalize( mean=[_ ,_ ,_ ],std = [_ ,_ ,_ ] ) normalizes the input by subtracting the mean and dividing by the standard deviation, the output is in the range [-1,1]. It is important to apply the specified mean and std when using a pre-trained model. To get the original image back use . image = ((image * std) + mean) . For example, when using a model trained on ImageNet it is common to apply this transformation. It normalizes the data to have a mean of ~0 and std of ~1 . transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]) . torchvision.transforms vs torchvision.transforms.functional. . The functional API is stateless and you can directly pass all the necessary arguments. Whereas torchvision.transforms are classes initialized with some default parameters unless specified. . # Class-based. Define once and use multiple times transform = transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) data = transform(data) # Functional. Pass parameters each time data = TF.normalize(data, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) . The functional API is very useful when transforming your data and target with the same random values, e.g. random cropping: . i, j, h, w = transforms.RandomCrop.get_params(image, output_size=(512, 512)) image = TF.crop(image, i, j, h, w) mask = TF.crop(mask, i, j, h, w) . Functional API also allows us to perform identical transforms on both image and target . def transform(self, image, mask): # Resize resize = transforms.Resize(size=(520, 520)) image = resize(image) mask = resize(mask # Random horizontal flipping if random.random() &gt; 0.5: image = TF.hflip(image) mask = TF.hflip(mask) # Random vertical flipping if random.random() &gt; 0.5: image = TF.vflip(image) mask = TF.vflip(mask) . Data Augmentation happens at the step below. At this point, __getitem__ method in the Dataset Class is called, and the transformations are applied. . for data in train_loader(): . Writing Custom Autograd Functions . Example . class MyReLU(torch.autograd.Function): @staticmethod def forward(ctx, i): input = i.clone() &quot;&quot;&quot; ctx is a context object that can be used to stash information for backward computation. You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method. &quot;&quot;&quot; ctx.save_for_backward(input) return input.clamp(min=0) @staticmethod def backward(ctx, grad_output): &quot;&quot;&quot; In the backward pass we receive a Tensor containing the gradient of the loss wrt the output, and we need to compute the gradient of the loss wrt the input. &quot;&quot;&quot; input, = ctx.saved_tensors grad_input = grad_output.clone() grad_input[input &lt; 0] = 0 return grad_input . PyTorch Examples for Reference Github . PyTorch official docs . Gradient returned by the class should have the same shape as the input to the class, to be able to update the input in the optimizer.step() function. . Avoid using in-place operations as they cause problems while back-propagation because of the way they modify the graph. As a precaution, always clone the input in the forward pass, and clone the incoming gradients before modifying them. . An in-place operation directly modifies the content of a given Tensor without making a copy. Inplace operations in PyTorch are always postfixed with a , like .add() or .scatter_(). Python operations like + = or *= are also in-place operations. . grad_input = grad_output.clone() return grad_input . Dealing with non-differentiable functions: . w_hard : non-differentiable w_soft : differentiable proxy for w_hard . w_bar = w_soft + tf.stop_grad(w_hard - w_soft) #in tensorflow w_bar = w_soft + (w_hard - w_soft).detach() #in PyTorch . It gets you x_forward in the forward pass, but derivative acts as if you had x_backward . y = x_backward + (x_forward - x_backward).detach() . loss.backward() computes d(loss)/d(w) for every parameter which has requires_grad=True. They are accumulated in w.grad. And the optimizer.step() updates w using w.grad, w += -lr* x.grad . Saving and Loading Models . PyTorch saves models as a state_dict. . torch.save({ &#39;encoder_state_dict&#39;: encoder.state_dict(), &#39;decoder_state_dict&#39;: decoder.state_dict() },os.path.join(args.experiment_dir,&quot;latest_checkpoint.tar&quot;)) . Use keyword strict when you have added new layers to the architecture which were not present in the model you saved as checkpoint . encoder = Encoder() checkpoint = torch.load(&#39;checkpoints/clic.tar&#39;) encoder.load_state_dict(checkpoint[&#39;encoder_state_dict&#39;], strict=False) . On Loading a model, if it shows a message like this, it means there were no missing keys (it’s not an error). . IncompatibleKeys(missing_keys=[], unexpected_keys=[]) . Keyboard interrupt and saving the last state of a model: . try: # training code here except KeyboardInterrupt: # save model here . Extra Readings . Grokking PyTorch | Effective PyTorch | The Python Magic Behind PyTorch | Python is Cool - ChipHuyen | PyTorch StyleGuide | Clean Code Python | Using _ in Variable Naming | Pytorch Coding Conventions | Fine Tuning etc | . More Tutorials . https://github.com/dsgiitr/d2l-pytorch | https://github.com/L1aoXingyu/pytorch-beginner | https://github.com/yunjey/pytorch-tutorial | https://github.com/MorvanZhou/PyTorch-Tutorial | .",
            "url": "https://adityassrana.github.io/blog/tutorials/2020/01/29/PyTorch-Tips.html",
            "relUrl": "/tutorials/2020/01/29/PyTorch-Tips.html",
            "date": " • Jan 29, 2020"
        }
        
    
  

  
  

  

  

  
      ,"page3": {
          "title": "Resources",
          "content": "Coming Soon .",
          "url": "https://adityassrana.github.io/blog/resources/",
          "relUrl": "/resources/",
          "date": ""
      }
      
  

  
  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://adityassrana.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}