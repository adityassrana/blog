{
  
    
        "post0": {
            "title": "Don't Trust PyTorch to Initialize Your Variables",
            "content": "This post is the summary of things I learnt while doing Fast.ai&#39;s Deep Learning from Foundations and Justin Jonshons&#39;s Computer Vision Course which is the updated and latest version of CS321n. . Situation . You want to solve a problem using deep learning. You have collected a dataset, decided on a neural network architecture, loss function, optimizer and some metrics you want to evaluate on. While training you notice your network isn&#39;t performing well, neither on train nor test dataset. Looking for bugs while training neural networks is not a simple task, so we break down the whole training process into separate pipelines. Let&#39;s start by looking for bugs in our architecture and the way we initialize our weights. . Problem : Why does good initialization matter and when do gradients vanish? . As Neural Networks involve a lot of matrix multiplications, the mean and variance of activations can quickly shoot off to very high values or drop down to zero. This will cause the local gradients of our layers to become NaN or zero and hence prevent our network from learning anything . A common strategy to avoid this is to initialize the weights of your network using the latest techniques. For example if you’re using ReLU activation after a layer, you must initialize your weights with Kaiming He initialization and set the biases to zero.(This was introduced in the 2014 ImageNet winning paper from Microsoft). This ensures the mean and standard deviation of activations of all layers stay close to 0 and 1 respectively. . . Upstream gradients are multiplied by local gradients to get the downstream gradients during backpropLet&#39;s compare the local gradient behavior of some common activation functions . . Notice how the gradients in both sigmoid and tanh are non-zero only inside a certain range between [-5, 5] . Also notice that when using sigmoid, the local gradient achieves a maximum value of 0.25, thus every time gradient passes through a sigmoid layer, it gets diminished by at least 75%. . Task . To ensure the gradients of our network do not explode or diminish to zero . The Mean of activations should be zero | The Variance of activations should stay same across layers. | Intitalizing the network with the right weights can make the difference between converging in a few epochs versus not converging at all. . Solution: Let&#39;s Compare Differrent Intialization Strategies . You must be wondering that surely it cannot be that bad. Let&#39;s consider a forward pass for a 6-layer neural network with each hidden layer the size of 4096 and tanh activation and let&#39;s plot the histogram for activations after each layer . All Weights and Biases Set to Zero . . Image SourceDo not ever do this! In such a case, all neurons of a layer would end up computing the same output, will receive the same gradients during backpropagation and undergo the same parameter updates. That means that all neurons of a layer will be learning the same features during training as there is no symmetry breaking. . import numpy as np import matplotlib.pyplot as plt %matplotlib inline . plt.style.use(&#39;seaborn&#39;) . Using Small Random Numbers from a Normal Distribution . Why Normal/Gaussian? Because it is characterized by its mean and variance, exactly the two thing we want to control and compare . dims = [4096]*7 mean = [] std = [] hist = [] x = np.random.randn(dims[0]) for Din, Dout in zip(dims[:-1],dims[1:]): W = 0.01*np.random.randn(Din,Dout) x = np.tanh(x.dot(W)) print(x.shape) mean.append(x.mean()) std.append(x.std()) hist.append(x) fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 8)) for i,ax,m,s in zip(np.arange(6),axes.flatten(),mean,std): ax.hist(hist[i],density = True,bins=50) ax.set_xlim(-1.2,1.2) ax.set_title(f&#39;Layer:{i+1} mean:{m:.2f} and std;{s:.2f}&#39;) plt.tight_layout() . (4096,) (4096,) (4096,) (4096,) (4096,) (4096,) . . As you can see above, the gradient is zero when sigmoid output is zero. By the last layer most activations become zero, and hence no learning. Clearly this approach did not work. . fig, ax = plt.subplots(figsize=(3,3)) ax.hist(hist[5],bins=50) ax.set_xlim(-1,1) plt.tight_layout() . The same activations as above super-imposed on each other. Plotting this just because it seems visually appealing to me . plt.style.use(&#39;default&#39;) for m,s in zip(mean,std): a = np.random.normal(m,s,size=1000) plt.hist(a,bins=50) . plt.style.use(&#39;seaborn&#39;) . Maybe larger weights will not get diminished . dims = [4096]*7 mean = [] std = [] hist = [] x = np.random.randn(dims[0]) for Din, Dout in zip(dims[:-1],dims[1:]): W = 0.5*np.random.randn(Din,Dout) x = np.tanh(x.dot(W)) print(x.shape) mean.append(x.mean()) std.append(x.std()) hist.append(x) fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 8)) for i,ax,m,s in zip(np.arange(6),axes.flatten(),mean,std): ax.hist(hist[i],density = True,bins=50) ax.set_xlim(-1.2,1.2) ax.set_title(f&#39;Layer:{i+1} mean:{m:.2f} and std;{s:.2f}&#39;) plt.tight_layout() . (4096,) (4096,) (4096,) (4096,) (4096,) (4096,) . Wow, that was a disaster. All neurons are saturated and are outputting -1s and 1s where gradient is zero. Again, no learning . Xavier : Using a magic scaling number for our normal distribution . $$ sqrt{ frac{1}{Din}}$$ . dims = [4096]*7 mean = [] std = [] hist = [] x = np.random.randn(dims[0]) for Din, Dout in zip(dims[:-1],dims[1:]): W = np.random.randn(Din,Dout)/np.sqrt(Din) x = np.tanh(x.dot(W)) print(x.shape) mean.append(x.mean()) std.append(x.std()) hist.append(x) fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 8)) for i,ax,m,s in zip(np.arange(6),axes.flatten(),mean,std): ax.hist(hist[i],density = True,bins=50) ax.set_xlim(-1.2,1.2) ax.set_title(f&#39;Layer:{i+1} mean:{m:.2f} and std;{s:.2f}&#39;) plt.tight_layout() . (4096,) (4096,) (4096,) (4096,) (4096,) (4096,) . Hmmmmmmmmm, why did that work? This was proposed in Understanding the difficulty of training deep feedforward neural networks, the 2010 paper that introduced ‘Xavier initialization’ which we have just used. . But where does it come from? It&#39;s not that mysterious if you remember the definition of the matrix multiplication. When we do y = x @ W, the coefficients of y are defined by . $$y_{i} = x_{1} w_{1} + x_{2} w_{2} + cdots + x_{n} w_{n} = sum_{j=1}^{Din} x_{j} w_{j} $$ . Now at the very beginning, our x vector has a mean of roughly 0. and a standard deviation of roughly 1. (since we picked it that way). . $$ Var(y_{i}) = Var(x_{1} w_{1} + x_{2} w_{2} + cdots + x_{n} w_{n}) $$ . Assume x, w are iid $$ Var(y_{i}) = D_{in} * Var(x_{i} w_{i}) $$ | Assume x, w independent $$ Var(y_{i}) = D_{in} * (E[x_{i}^2] E[w_{i}^2] - E[x_{i}]^2 E[w_{i}]^2) $$ | Assume x, w are zero-mean $$ Var(y_{i}) = D_{in} * Var(x_{i}) * Var(w_{i}) $$ | Hence the condition for the variances to remain same that is $$ Var(y_{i}) = Var(x_{i})$$ the condition is $$ Var(w_i) = frac{1}{Din} $$ . That&#39;s it, the Xavier initialization. . Kaiming Initialization, if using ReLU scale by $$ sqrt{ frac{2}{Din}}$$ . Xavier initialization assumes the input to have zero mean, but things change when we use a ReLU which sets all negative values to zero. Let&#39;s see what happens if we continue using Xavier initialization with ReLU . dims = [4096]*7 mean = [] std = [] hist = [] x = np.random.randn(dims[0]) for Din, Dout in zip(dims[:-1],dims[1:]): W = np.random.randn(Din,Dout)/np.sqrt(Din) x = np.maximum(0, x.dot(W)) print(x.shape) mean.append(x.mean()) std.append(x.std()) hist.append(x) fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 8)) for i,ax,m,s in zip(np.arange(6),axes.flatten(),mean,std): ax.hist(hist[i],density = True,bins=50) ax.set_xlim(-1.2,1.5) ax.set_title(f&#39;Layer:{i+1} mean:{m:.2f} and std;{s:.2f}&#39;) plt.tight_layout() . (4096,) (4096,) (4096,) (4096,) (4096,) (4096,) . Things aren&#39;t looking too good. Let&#39;s try out Kaiming now . dims = [4096]*7 mean = [] std = [] hist = [] x = np.random.randn(dims[0]) for Din, Dout in zip(dims[:-1],dims[1:]): W = np.random.randn(Din,Dout)/np.sqrt(Din/2) x = np.maximum(0, x.dot(W)) print(x.shape) mean.append(x.mean()) std.append(x.std()) hist.append(x) fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 8)) for i,ax,m,s in zip(np.arange(6),axes.flatten(),mean,std): ax.hist(hist[i],density = True,bins=50) ax.set_xlim(-1.2,1.5) ax.set_title(f&#39;Layer:{i+1} mean:{m:.2f} and std;{s:.2f}&#39;) plt.tight_layout() . (4096,) (4096,) (4096,) (4096,) (4096,) (4096,) . Things are looking much better now . Here&#39;s a sample code if you get confused how to add multiple plots to the same figure. You can read more about this here . #collapse-hide xs = np.linspace(0, 2 * np.pi, 400) ys = np.sin(xs ** 2) xc = np.linspace(0, 2 * np.pi, 600) yc = np.cos(xc ** 2) fig, ax = plt.subplots(2, 2, figsize=(16, 8)) # `Fig` is short for Figure. `ax` is short for Axes. ax[0, 0].plot(xs, ys) ax[1, 1].plot(xs, ys) ax[0, 1].plot(xc, yc) ax[1, 0].plot(xc, yc) fig.suptitle(&quot;Basic plotting using Matplotlib&quot;) plt.show() . . How to calculate fan-in and fan-out in Xavier initialization for CNNs? . While reading the papers on initialization you&#39;ll come across these two terms quite often. . This part of my post is heavily inspired by Matthew Kleinsmith&#39;s post on CNN Visualizations on Medium . . Similarly, a Conv Layer can be seen as a Linear layer. . . The Image . The Filter . Since the filter fits in the image four times, we have four results . Here’s how we applied the filter to each section of the image to yield each result . The equation view . The compact equation viewand now most importantly the neural network view where you can see each output is generated from 4 inputs and hence fan_in = 4. . . If the original image had been a 3-channel image, each output would be generated from 3*4 = 12 inputs and hence fan_in would be 12. Hence, . receptive_field_size = kernel_height * kernel_width fan_in = num_input_feature_maps * receptive_field_size fan_out = num_output_feature_maps * receptive_field_size . I would also encourage you to play around with the PyTorch functions for calculating fan_in and fan_out here.. Somewhat like this, referring to the example above . import torch . conv = torch.nn.Conv2d(in_channels=1,out_channels=1,kernel_size=2) . conv.weight.shape . torch.Size([1, 1, 2, 2]) . torch.nn.init._calculate_fan_in_and_fan_out(conv.weight) . (4, 4) . conv.weight.numel() . 4 . conv.weight.mean(),conv.weight.std() . (tensor(-0.0338, grad_fn=&lt;MeanBackward0&gt;), tensor(0.3015, grad_fn=&lt;StdBackward0&gt;)) . torch.nn.init.kaiming_normal_(conv.weight,nonlinearity=&#39;relu&#39;); . conv.weight.mean(), conv.weight.std() . (tensor(0.5565, grad_fn=&lt;MeanBackward0&gt;), tensor(1.4324, grad_fn=&lt;StdBackward0&gt;)) . Don&#39;t worry seeing a high mean above, it&#39;s because our sample size is small as our filter only has 4 elements. In a normal convolution filter, this wouldn&#39;t be the case . Play around with your own network . deeplearning.ai has built an amazing interactive tool to test out all the things we have discussed above. Go check it out at https://www.deeplearning.ai/ai-notes/initialization/ . Okay, now why can&#39;t we trust PyTorch to initialize our weights for us by default? . I&#39;ve recently discovered that PyTorch does not use modern/recommended weight initialization techniques by default when creating Conv/Linear Layers. They&#39;ve been doing it using the old strategies so as to maintain backward compatibility in their code. I know it sounds strange, weird and very stupid but unfortunately it&#39;s true. As of 26th August 2020, this issue is still open on Github . Bug . torch.nn.modules.conv._ConvNd.reset_parameters?? . Signature: torch.nn.modules.conv._ConvNd.reset_parameters(self) Docstring: &lt;no docstring&gt; Source: def reset_parameters(self): init.kaiming_uniform_(self.weight, a=math.sqrt(5)) if self.bias is not None: fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight) bound = 1 / math.sqrt(fan_in) init.uniform_(self.bias, -bound, bound) File: ~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/conv.py Type: function . The above bug exists because PyTorch was adapted from Torch library, and authors found sqrt(5) to work well, but there&#39;s no justification or intuition behind this. Surprisingly, Tensorflow also uses the Glorot initialization for Conv2d by default as well, which is again suboptimal when working with ReLU. . However, when PyTorch provides pretrained resnet and other architecture models, they cover up for this by explicitly initializing layers in the code with kaiming normal. You can see an example here. So this means . If you&#39;re importing a network from torchvision, it was initialized properly and there is nothing to worry about but | If you cut some of its layers and replace it with your own layers, you need to initialize them again as per recommended methods which is kaiming normal if you&#39;re working with ReLU function. | If you&#39;re writing your own layers which is what happens 99% of the case, initialize them explicitly | Solution . For example, in residual networks example given above the following code is used . for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode=&#39;fan_out&#39;, nonlinearity=&#39;relu&#39;) elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) # Zero-initialize the last BN in each residual branch, # so that the residual branch starts with zeros, and each residual block behaves like an identity. # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677 if zero_init_residual: for m in self.modules(): if isinstance(m, Bottleneck): nn.init.constant_(m.bn3.weight, 0) elif isinstance(m, BasicBlock): nn.init.constant_(m.bn2.weight, 0) . Weight Initialization: Residual Networks . . Problem : If we initialize with Kaiming: then Var(F(x)) = Var(x). But now Var(F(x) + x) will be greater than Var(x) – variance grows with each block! . Solution: Initialize first conv with Kaiming, initialize second conv to zero. Then Var(x + F(x)) = Var(x) . References And More Links . 3Blue1Brown Neural Networks Playlist: If you&#39;re new to all this and confused about what networks have to do with linear algebra | Slides by Justin Johnson and CS231n | Yes you should understand backprop by Andrej Karpathy | https://pouannes.github.io/blog/initialization/ | https://jimypbr.github.io/2020/02/fast-ai-lesson-8-notes-backprop-from-the-foundations | https://medium.com/comet-ml/selecting-the-right-weight-initialization-for-your-deep-neural-network-780e20671b22 |",
            "url": "https://adityassrana.github.io/blog/theory/2020/08/26/Weight_Init.html",
            "relUrl": "/theory/2020/08/26/Weight_Init.html",
            "date": " • Aug 26, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Vectorizing Nearest Neighbours Algorithm",
            "content": "This problem is similar to Stanford&#39;s kNN classifier assignment which implements this algorithm on the CIFAR-10 dataset . . Situation . In a hypothetical $n$-dimensional universe, there exists $p$ population of a particular species of humans, Homo BITSians. This species likes to hangout in specialized eateries, called Redis. In this universe, there are $q$ Redis which serve delicious snacks and beverages at nominal prices. Our task is to find the nearest Redi from each of the Homo BITSians so that they spend less time on commuting. . Problem . Matrices, $X in p times n$ and $Y in q times n$, which have the co-ordinates of $p$ Homo BITSians and $q$ Redis respectively in the $n$-dimensional universe are given. The $i^{th}$ row in the matrix, $X$, corresponds to the $i^{th}$ Homo BITSian. Similarly, the $i^{th}$ row in the matrix, $Y$, corresponds to the $i^{th}$ Redi. . Note: Here, row numbering (indexing) starts from $0$. . Task . Given $X$, $Y$, find a vector, $V$, of length $p$. The vector, $V$, is such that the $i^{th}$ element of $V$ has the index of the nearest Redi from the $i^{th}$ Homo BITSian. . Distance metric is the usual $l_2$-norm. In a n-dimensional space with points $x = (x_0, x_0, ldots, x_{n-1})$ and $y = (y_0, y_0, ldots, y_{n-1})$, the distance can be calculated as: . $$D_{xy}^2 = (x_0 - y_0)^2 + (x_1 - y_1)^2 + ldots + (x_{n-1} - y_{n-1})^2$$ Part 1: Find the index of the nearest Redi from each Homo BITSian . # Base Distance Function to be completed by the student import numpy as np def distances(X, Y): &quot;&quot;&quot; Given matrices X and Y, the function returns a distance matrix. The (i,j)th element of the matrix contains the distance of jth Redi from the ith Homo BITSian. Parameters: X,Y Returns: D &quot;&quot;&quot; ### BEGIN SOLUTION ### END SOLUTION return D . def nearest_redi(X, Y): &quot;&quot;&quot; Given matrices X and Y, the function returns a nearest redi vector. The i-th element of the vector contains the index of nearest Redi from the ith Homo BITSian. Parameters: X,Y Returns: V &quot;&quot;&quot; D = distances(X,Y) ### BEGIN SOLUTION ### END SOLUTION return V . Solutions begin from here . The way to understand the axis argument in numpy functions is that it collapses the specified axis. So when we specify the axis 1 (the column), it applies the function across all columns, resulting in a single column.For more intuition check out this post by Aerin Kim. In fact, I would reccommend you to read all of her posts. . def nearest_redi(X, Y): D = distances(X, Y) V = np.argmin(D, 1) return V . Let&#39;s evaluate the time taken by different approaches . X = np.random.randn(100,1024) Y = np.random.randn(1000,1024) . single loop using matrix addition/subtraction broadcast . def distances(X, Y): dist = np.zeros((X.shape[0], Y.shape[0])) for i,x in enumerate(X): dist[i] = ((X[i] - Y)**2).sum(axis=1) return dist %time test1 = nearest_redi(X, Y) . CPU times: user 166 ms, sys: 92 µs, total: 166 ms Wall time: 165 ms . no loops, using a gigantic outer product by forming a 3-d matrix of distances. Neat but still inefficient. Explained here . def distances(X, Y): x = X.reshape(X.shape[0], 1, X.shape[1]) dist = ((x - Y)**2).sum(axis = 2) return dist %time test2 = nearest_redi(X, Y) . CPU times: user 295 ms, sys: 96.3 ms, total: 392 ms Wall time: 390 ms . no loop, but breaking up the L2 norm into individual terms . $$ left | mathbf{I_1} - mathbf{I_2} right | = sqrt{ left | mathbf{I_1} right |^2 + left | mathbf{I_2} right | ^2 - 2 mathbf{I_1} cdot mathbf{I_2}} $$ . def distances(X, Y): dist = np.zeros((X.shape[0], Y.shape[0])) dist = (X**2).sum(axis=1)[:, np.newaxis] + (Y**2).sum(axis=1) - 2 * X.dot(Y.T) return dist %time test3 = nearest_redi(X, Y) . CPU times: user 12.8 ms, sys: 888 µs, total: 13.7 ms Wall time: 4.58 ms . Test Your Algorithm . #collapse-hide print(&quot;Running base test case 1...&quot;) X_test1 = np.array([[-3., 4.], [ 4., -2.], [-1., 0.]]) Y_test1 = np.array([[-3., 0.], [-3., -3.]]) V_test1 = nearest_redi(X_test1, Y_test1) V_ans_test1 = np.array([0, 1, 0]) assert np.array_equal(V_test1, V_ans_test1) print(&quot;Base test case 1 successful!! n&quot;) print(&quot;Running base test case 2...&quot;) X_test2 = np.array([[ 0.08170274, -4.8955951 , -4.0473417 ], [-1.13259313, 4.38171415, -3.22068891]]) Y_test2 = np.array([[ 3.79010736, 1.70042849, -3.06603884], [ 3.8921235 , -1.85207272, 2.33340715], [ 1.67360485, 2.11437547, 0.87529999]]) V_test2 = nearest_redi(X_test2, Y_test2) V_ans_test2 = np.array([0, 2]) assert np.array_equal(V_test2, V_ans_test2) print(&quot;Base test case 2 successful!! n&quot;) . . Running base test case 1... Base test case 1 successful!! Running base test case 2... Base test case 2 successful!! . #collapse-hide # Running hidden test case for Part 1. Don&#39;t edit the cell. *** 5 marks *** ### BEGIN HIDDEN TESTS X = np.array([[ 0.27170746, 0.89441607, 0.64849028], [ 0.42296173, 0.54342876, 0.47889235], [ 0.48688657, 0.11082849, 0.10691689], [ 0.04419385, 0.68777309, 0.49437059], [ 0.70143641, 0.09964604, 0.20949214], [ 0.01725016, 0.37424641, 0.94070338]]) Y = np.array([[ 0.24232741, 0.08413896, 0.014919 ], [ 0.15801316, 0.31713579, 0.0416702 ], [ 0.15784176, 0.50998073, 0.45405793], [ 0.44382259, 0.44515729, 0.49186482], [ 0.00695024, 0.23603969, 0.77601819]]) V = nearest_redi(X,Y) V_ans = np.array([2, 3, 0, 2, 0, 4]) assert np.array_equal(V, V_ans) ### END HIDDEN TESTS . . Extra Resources . https://www.labri.fr/perso/nrougier/from-python-to-numpy/ - Read the sections on Code Vectorization and Problem Vectorization | https://medium.com/@mikeliao/numpy-vectorization-d4adea4fc2a | https://mlxai.github.io/2017/01/03/finding-distances-between-data-points-with-numpy.html |",
            "url": "https://adityassrana.github.io/blog/broadcasting/numpy/2020/07/24/Vectorization.html",
            "relUrl": "/broadcasting/numpy/2020/07/24/Vectorization.html",
            "date": " • Jul 24, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://adityassrana.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "PyTorch Tips and Tricks",
            "content": "PyTorch . Input Tensor Format : (N,C,H,W). The model and the convolutional layers expect the input tensor to be of this shape, so when feeding an image/images to the model, add a dimension for batching. . Converting from img–&gt;numpy representation and feeding the model gives an error because the input is in ByteTensor format. Only float operations are supported for conv-like operations. . img = img.type(&#39;torch.DoubleTensor&#39;) . Dataset and Transforms . Dataset Class : what data will be input to the model and what augmentations will be applied | DataLoader Class : how big a minibatch will be, | . To create our own dataset class in PyTorch we inherit from the Dataset Class and define two main methods, the __len__ and the __getitem__ . import torch from PIL import Image import torchvision import torchvision.transforms.functional as TF #it&#39;s not tensorflow class ImageDataset(torch.utils.data.Dataset): &quot;&quot;&quot;Dataset class for creating data pipeline for images&quot;&quot;&quot; def __init__(self, train_glob, patchsize): &quot;&quot;&quot;&quot; train_glob is a Glob pattern identifying training data. This pattern must expand to a list of RGB images in PNG format. for eg. &quot;/images/cat/*.png&quot; patchsize is the crop size you want from the image &quot;&quot;&quot; self.list_id = glob.glob(train_glob) self.patchsize = patchsize def __len__(self): #denotes total number of samples return len(self.list_id) def __getitem__(self, index): #generates one sample of data image = Image.open(self.list_id[index]) # convert to RGB if image is B/W if image.mode == &#39;L&#39;: image = image.convert(&#39;RGB&#39;) image= self.transform(image) return image def transform(self,image): # Fucntional transforms allow us to apply # the same crop on semantic segmentation i, j, h, w = torchvision.transforms.RandomCrop.get_params(image , output_size = (self.patchsize, self.patchsize)) image = TF.crop(image, i, j, h, w) image = TF.to_tensor(image) return image . Image processing operations like cropping and resizing should be done on the PIL Image and not the tensor . Image --&gt; Crop/Resize --&gt; toTensor --&gt; Normalize . The transforms.ToTensor() or TF.to_tensor (functional version of the same command) separates the PIL Image into 3 channels (R,G,B), converts it to the range (0,1). You can multiply by 255 to get the range (0,255). . Using transforms.Normalize( mean=[_ ,_ ,_ ],std = [_ ,_ ,_ ] ) normalizes the input by subtracting the mean and dividing by the standard deviation, the output is in the range [-1,1]. It is important to apply the specified mean and std when using a pre-trained model. To get the original image back use . image = ((image * std) + mean) . For example, when using a model trained on ImageNet it is common to apply this transformation. It normalizes the data to have a mean of ~0 and std of ~1 . transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]) . torchvision.transforms vs torchvision.transforms.functional. . The functional API is stateless and you can directly pass all the necessary arguments. Whereas torchvision.transforms are classes initialized with some default parameters unless specified. . # Class-based. Define once and use multiple times transform = transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) data = transform(data) # Functional. Pass parameters each time data = TF.normalize(data, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) . The functional API is very useful when transforming your data and target with the same random values, e.g. random cropping: . i, j, h, w = transforms.RandomCrop.get_params(image, output_size=(512, 512)) image = TF.crop(image, i, j, h, w) mask = TF.crop(mask, i, j, h, w) . Functional API also allows us to perform identical transforms on both image and target . def transform(self, image, mask): # Resize resize = transforms.Resize(size=(520, 520)) image = resize(image) mask = resize(mask # Random horizontal flipping if random.random() &gt; 0.5: image = TF.hflip(image) mask = TF.hflip(mask) # Random vertical flipping if random.random() &gt; 0.5: image = TF.vflip(image) mask = TF.vflip(mask) . Data Augmentation happens at the step below. At this point, __getitem__ method in the Dataset Class is called, and the transformations are applied. . for data in train_loader(): . Writing Custom Autograd Functions . Example . class MyReLU(torch.autograd.Function): @staticmethod def forward(ctx, i): input = i.clone() &quot;&quot;&quot; ctx is a context object that can be used to stash information for backward computation. You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method. &quot;&quot;&quot; ctx.save_for_backward(input) return input.clamp(min=0) @staticmethod def backward(ctx, grad_output): &quot;&quot;&quot; In the backward pass we receive a Tensor containing the gradient of the loss wrt the output, and we need to compute the gradient of the loss wrt the input. &quot;&quot;&quot; input, = ctx.saved_tensors grad_input = grad_output.clone() grad_input[input &lt; 0] = 0 return grad_input . PyTorch Examples for Reference Github . PyTorch official docs . Gradient returned by the class should have the same shape as the input to the class, to be able to update the input in the optimizer.step() function. . Avoid using in-place operations as they cause problems while back-propagation because of the way they modify the graph. As a precaution, always clone the input in the forward pass, and clone the incoming gradients before modifying them. . An in-place operation directly modifies the content of a given Tensor without making a copy. Inplace operations in PyTorch are always postfixed with a , like .add() or .scatter_(). Python operations like + = or *= are also in-place operations. . grad_input = grad_output.clone() return grad_input . Dealing with non-differentiable functions: . w_hard : non-differentiable w_soft : differentiable proxy for w_hard . w_bar = w_soft + tf.stop_grad(w_hard - w_soft) #in tensorflow w_bar = w_soft + (w_hard - w_soft).detach() #in PyTorch . It gets you x_forward in the forward pass, but derivative acts as if you had x_backward . y = x_backward + (x_forward - x_backward).detach() . loss.backward() computes d(loss)/d(w) for every parameter which has requires_grad=True. They are accumulated in w.grad. And the optimizer.step() updates w using w.grad, w += -lr* x.grad . Saving and Loading Models . PyTorch saves models as a state_dict. . torch.save({ &#39;encoder_state_dict&#39;: encoder.state_dict(), &#39;decoder_state_dict&#39;: decoder.state_dict() },os.path.join(args.experiment_dir,&quot;latest_checkpoint.tar&quot;)) . Use keyword strict when you have added new layers to the architecture which were not present in the model you saved as checkpoint . encoder = Encoder() checkpoint = torch.load(&#39;checkpoints/clic.tar&#39;) encoder.load_state_dict(checkpoint[&#39;encoder_state_dict&#39;], strict=False) . On Loading a model, if it shows a message like this, it means there were no missing keys (it’s not an error). . IncompatibleKeys(missing_keys=[], unexpected_keys=[]) . Keyboard interrupt and saving the last state of a model: . try: # training code here except KeyboardInterrupt: # save model here . Extra Readings . Grokking PyTorch | Effective PyTorch | The Python Magic Behind PyTorch | Python is Cool - ChipHuyen | PyTorch StyleGuide | Clean Code Python | Using _ in Variable Naming | Pytorch Coding Conventions | Fine Tuning etc | . More Tutorials . https://github.com/dsgiitr/d2l-pytorch | https://github.com/L1aoXingyu/pytorch-beginner | https://github.com/yunjey/pytorch-tutorial | https://github.com/MorvanZhou/PyTorch-Tutorial | .",
            "url": "https://adityassrana.github.io/blog/tutorials/2020/01/29/PyTorch-Tips.html",
            "relUrl": "/tutorials/2020/01/29/PyTorch-Tips.html",
            "date": " • Jan 29, 2020"
        }
        
    
  

  
  

  

  

  
      ,"page3": {
          "title": "Resources",
          "content": "Coming Soon .",
          "url": "https://adityassrana.github.io/blog/resources/",
          "relUrl": "/resources/",
          "date": ""
      }
      
  

  
  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://adityassrana.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}