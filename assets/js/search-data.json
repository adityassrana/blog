{
  
    
        "post0": {
            "title": "Vectorizing Nearest Neighbours Algorithm",
            "content": "This problem is similar to Stanford&#39;s kNN classifier assignment which implements this algorithm on the CIFAR-10 dataset . . Situation . In a hypothetical $n$-dimensional universe, there exists $p$ population of a particular species of humans, Homo BITSians. This species likes to hangout in specialized eateries, called Redis. In this universe, there are $q$ Redis which serve delicious snacks and beverages at nominal prices. Our task is to find the nearest Redi from each of the Homo BITSians so that they spend less time on commuting. . Problem . Matrices, $X in p times n$ and $Y in q times n$, which have the co-ordinates of $p$ Homo BITSians and $q$ Redis respectively in the $n$-dimensional universe are given. The $i^{th}$ row in the matrix, $X$, corresponds to the $i^{th}$ Homo BITSian. Similarly, the $i^{th}$ row in the matrix, $Y$, corresponds to the $i^{th}$ Redi. . Note: Here, row numbering (indexing) starts from $0$. . Task . Given $X$, $Y$, find a vector, $V$, of length $p$. The vector, $V$, is such that the $i^{th}$ element of $V$ has the index of the nearest Redi from the $i^{th}$ Homo BITSian. . Distance metric is the usual $l_2$-norm. In a n-dimensional space with points $x = (x_0, x_0, ldots, x_{n-1})$ and $y = (y_0, y_0, ldots, y_{n-1})$, the distance can be calculated as: . $$D_{xy}^2 = (x_0 - y_0)^2 + (x_1 - y_1)^2 + ldots + (x_{n-1} - y_{n-1})^2$$ Part 1: Find the index of the nearest Redi from each Homo BITSian . # Base Distance Function to be completed by the student import numpy as np def distances(X, Y): &quot;&quot;&quot; Given matrices X and Y, the function returns a distance matrix. The (i,j)th element of the matrix contains the distance of jth Redi from the ith Homo BITSian. Parameters: X,Y Returns: D &quot;&quot;&quot; ### BEGIN SOLUTION ### END SOLUTION return D . def nearest_redi(X, Y): &quot;&quot;&quot; Given matrices X and Y, the function returns a nearest redi vector. The i-th element of the vector contains the index of nearest Redi from the ith Homo BITSian. Parameters: X,Y Returns: V &quot;&quot;&quot; D = distances(X,Y) ### BEGIN SOLUTION ### END SOLUTION return V . Solutions begin from here . The way to understand the axis argument in numpy functions is that it collapses the specified axis. So when we specify the axis 1 (the column), it applies the function across all columns, resulting in a single column.For more intuition check out this post by Aerin Kim. In fact, I would reccommend you to read all of her posts. . def nearest_redi(X, Y): D = distances(X, Y) V = np.argmin(D, 1) return V . Let&#39;s evaluate the time taken by different approaches . X = np.random.randn(100,1024) Y = np.random.randn(1000,1024) . single loop using matrix addition/subtraction broadcast . def distances(X, Y): dist = np.zeros((X.shape[0], Y.shape[0])) for i,x in enumerate(X): dist[i] = ((X[i] - Y)**2).sum(axis=1) return dist %time test1 = nearest_redi(X, Y) . CPU times: user 276 ms, sys: 0 ns, total: 276 ms Wall time: 274 ms . no loops, using a gigantic outer product by forming a 3-d matrix of distances. Neat but still inefficient. Explained here . def distances(X, Y): x = X.reshape(X.shape[0], 1, X.shape[1]) dist = ((x - Y)**2).sum(axis = 2) return dist %time test2 = nearest_redi(X, Y) . CPU times: user 342 ms, sys: 104 ms, total: 446 ms Wall time: 443 ms . no loop, but breaking up the L2 norm into individual terms . $$ left | mathbf{I_1} - mathbf{I_2} right | = sqrt{ left | mathbf{I_1} right |^2 + left | mathbf{I_2} right | ^2 - 2 mathbf{I_1} cdot mathbf{I_2}} $$ . def distances(X, Y): dist = np.zeros((X.shape[0], Y.shape[0])) dist = (X**2).sum(axis=1)[:, np.newaxis] + (Y**2).sum(axis=1) - 2 * X.dot(Y.T) return dist %time test3 = nearest_redi(X, Y) . CPU times: user 40.5 ms, sys: 0 ns, total: 40.5 ms Wall time: 7.01 ms . Test Your Algorithm . #collapse-hide print(&quot;Running base test case 1...&quot;) X_test1 = np.array([[-3., 4.], [ 4., -2.], [-1., 0.]]) Y_test1 = np.array([[-3., 0.], [-3., -3.]]) V_test1 = nearest_redi(X_test1, Y_test1) V_ans_test1 = np.array([0, 1, 0]) assert np.array_equal(V_test1, V_ans_test1) print(&quot;Base test case 1 successful!! n&quot;) print(&quot;Running base test case 2...&quot;) X_test2 = np.array([[ 0.08170274, -4.8955951 , -4.0473417 ], [-1.13259313, 4.38171415, -3.22068891]]) Y_test2 = np.array([[ 3.79010736, 1.70042849, -3.06603884], [ 3.8921235 , -1.85207272, 2.33340715], [ 1.67360485, 2.11437547, 0.87529999]]) V_test2 = nearest_redi(X_test2, Y_test2) V_ans_test2 = np.array([0, 2]) assert np.array_equal(V_test2, V_ans_test2) print(&quot;Base test case 2 successful!! n&quot;) . . Running base test case 1... Base test case 1 successful!! Running base test case 2... Base test case 2 successful!! . #collapse-hide # Running hidden test case for Part 1. Don&#39;t edit the cell. *** 5 marks *** ### BEGIN HIDDEN TESTS X = np.array([[ 0.27170746, 0.89441607, 0.64849028], [ 0.42296173, 0.54342876, 0.47889235], [ 0.48688657, 0.11082849, 0.10691689], [ 0.04419385, 0.68777309, 0.49437059], [ 0.70143641, 0.09964604, 0.20949214], [ 0.01725016, 0.37424641, 0.94070338]]) Y = np.array([[ 0.24232741, 0.08413896, 0.014919 ], [ 0.15801316, 0.31713579, 0.0416702 ], [ 0.15784176, 0.50998073, 0.45405793], [ 0.44382259, 0.44515729, 0.49186482], [ 0.00695024, 0.23603969, 0.77601819]]) V = nearest_redi(X,Y) V_ans = np.array([2, 3, 0, 2, 0, 4]) assert np.array_equal(V, V_ans) ### END HIDDEN TESTS . . Extra Resources . https://www.labri.fr/perso/nrougier/from-python-to-numpy/ - Read the sections on Code Vectorization and Problem Vectorization | https://medium.com/@mikeliao/numpy-vectorization-d4adea4fc2a | https://mlxai.github.io/2017/01/03/finding-distances-between-data-points-with-numpy.html |",
            "url": "https://adityassrana.github.io/blog/broadcasting/numpy/2020/07/25/Vectorization.html",
            "relUrl": "/broadcasting/numpy/2020/07/25/Vectorization.html",
            "date": " • Jul 25, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://adityassrana.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "PyTorch Tips and Tricks",
            "content": "PyTorch . Input Tensor Format : (N,C,H,W). The model and the convolutional layers expect the input tensor to be of this shape, so when feeding an image/images to the model, add a dimension for batching. . Converting from img–&gt;numpy representation and feeding the model gives an error because the input is in ByteTensor format. Only float operations are supported for conv-like operations. . img = img.type(&#39;torch.DoubleTensor&#39;) . Dataset and Transforms . Dataset Class : what data will be input to the model and what augmentations will be applied | DataLoader Class : how big a minibatch will be, | . To create our own dataset class in PyTorch we inherit from the Dataset Class and define two main methods, the __len__ and the __getitem__ . import torch from PIL import Image import torchvision import torchvision.transforms.functional as TF #it&#39;s not tensorflow class ImageDataset(torch.utils.data.Dataset): &quot;&quot;&quot;Dataset class for creating data pipeline for images&quot;&quot;&quot; def __init__(self, train_glob, patchsize): &quot;&quot;&quot;&quot; train_glob is a Glob pattern identifying training data. This pattern must expand to a list of RGB images in PNG format. for eg. &quot;/images/cat/*.png&quot; patchsize is the crop size you want from the image &quot;&quot;&quot; self.list_id = glob.glob(train_glob) self.patchsize = patchsize def __len__(self): #denotes total number of samples return len(self.list_id) def __getitem__(self, index): #generates one sample of data image = Image.open(self.list_id[index]) # convert to RGB if image is B/W if image.mode == &#39;L&#39;: image = image.convert(&#39;RGB&#39;) image= self.transform(image) return image def transform(self,image): # Fucntional transforms allow us to apply # the same crop on semantic segmentation i, j, h, w = torchvision.transforms.RandomCrop.get_params(image , output_size = (self.patchsize, self.patchsize)) image = TF.crop(image, i, j, h, w) image = TF.to_tensor(image) return image . Image processing operations like cropping and resizing should be done on the PIL Image and not the tensor . Image --&gt; Crop/Resize --&gt; toTensor --&gt; Normalize . The transforms.ToTensor() or TF.to_tensor (functional version of the same command) separates the PIL Image into 3 channels (R,G,B), converts it to the range (0,1). You can multiply by 255 to get the range (0,255). . Using transforms.Normalize( mean=[_ ,_ ,_ ],std = [_ ,_ ,_ ] ) normalizes the input by subtracting the mean and dividing by the standard deviation, the output is in the range [-1,1]. It is important to apply the specified mean and std when using a pre-trained model. To get the original image back use . image = ((image * std) + mean) . For example, when using a model trained on ImageNet it is common to apply this transformation. It normalizes the data to have a mean of ~0 and std of ~1 . transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]) . torchvision.transforms vs torchvision.transforms.functional. . The functional API is stateless and you can directly pass all the necessary arguments. Whereas torchvision.transforms are classes initialized with some default parameters unless specified. . # Class-based. Define once and use multiple times transform = transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) data = transform(data) # Functional. Pass parameters each time data = TF.normalize(data, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) . The functional API is very useful when transforming your data and target with the same random values, e.g. random cropping: . i, j, h, w = transforms.RandomCrop.get_params(image, output_size=(512, 512)) image = TF.crop(image, i, j, h, w) mask = TF.crop(mask, i, j, h, w) . Functional API also allows us to perform identical transforms on both image and target . def transform(self, image, mask): # Resize resize = transforms.Resize(size=(520, 520)) image = resize(image) mask = resize(mask # Random horizontal flipping if random.random() &gt; 0.5: image = TF.hflip(image) mask = TF.hflip(mask) # Random vertical flipping if random.random() &gt; 0.5: image = TF.vflip(image) mask = TF.vflip(mask) . Data Augmentation happens at the step below. At this point, __getitem__ method in the Dataset Class is called, and the transformations are applied. . for data in train_loader(): . Writing Custom Autograd Functions . Example . class MyReLU(torch.autograd.Function): @staticmethod def forward(ctx, i): input = i.clone() &quot;&quot;&quot; ctx is a context object that can be used to stash information for backward computation. You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method. &quot;&quot;&quot; ctx.save_for_backward(input) return input.clamp(min=0) @staticmethod def backward(ctx, grad_output): &quot;&quot;&quot; In the backward pass we receive a Tensor containing the gradient of the loss wrt the output, and we need to compute the gradient of the loss wrt the input. &quot;&quot;&quot; input, = ctx.saved_tensors grad_input = grad_output.clone() grad_input[input &lt; 0] = 0 return grad_input . PyTorch Examples for Reference Github . PyTorch official docs . Gradient returned by the class should have the same shape as the input to the class, to be able to update the input in the optimizer.step() function. . Avoid using in-place operations as they cause problems while back-propagation because of the way they modify the graph. As a precaution, always clone the input in the forward pass, and clone the incoming gradients before modifying them. . An in-place operation directly modifies the content of a given Tensor without making a copy. Inplace operations in PyTorch are always postfixed with a , like .add() or .scatter_(). Python operations like + = or *= are also in-place operations. . grad_input = grad_output.clone() return grad_input . Dealing with non-differentiable functions: . w_hard : non-differentiable w_soft : differentiable proxy for w_hard . w_bar = w_soft + tf.stop_grad(w_hard - w_soft) #in tensorflow w_bar = w_soft + (w_hard - w_soft).detach() #in PyTorch . It gets you x_forward in the forward pass, but derivative acts as if you had x_backward . y = x_backward + (x_forward - x_backward).detach() . loss.backward() computes d(loss)/d(w) for every parameter which has requires_grad=True. They are accumulated in w.grad. And the optimizer.step() updates w using w.grad, w += -lr* x.grad . Saving and Loading Models . PyTorch saves models as a state_dict. . torch.save({ &#39;encoder_state_dict&#39;: encoder.state_dict(), &#39;decoder_state_dict&#39;: decoder.state_dict() },os.path.join(args.experiment_dir,&quot;latest_checkpoint.tar&quot;)) . Use keyword strict when you have added new layers to the architecture which were not present in the model you saved as checkpoint . encoder = Encoder() checkpoint = torch.load(&#39;checkpoints/clic.tar&#39;) encoder.load_state_dict(checkpoint[&#39;encoder_state_dict&#39;], strict=False) . On Loading a model, if it shows a message like this, it means there were no missing keys (it’s not an error). . IncompatibleKeys(missing_keys=[], unexpected_keys=[]) . Keyboard interrupt and saving the last state of a model: . try: # training code here except KeyboardInterrupt: # save model here . Extra Readings . Grokking PyTorch | Effective PyTorch | The Python Magic Behind PyTorch | Python is Cool - ChipHuyen | PyTorch StyleGuide | Clean Code Python | Using _ in Variable Naming | Pytorch Coding Conventions | Fine Tuning etc | . More Tutorials . https://github.com/dsgiitr/d2l-pytorch | https://github.com/L1aoXingyu/pytorch-beginner | https://github.com/yunjey/pytorch-tutorial | https://github.com/MorvanZhou/PyTorch-Tutorial | .",
            "url": "https://adityassrana.github.io/blog/tutorials/2020/01/29/PyTorch-Tips.html",
            "relUrl": "/tutorials/2020/01/29/PyTorch-Tips.html",
            "date": " • Jan 29, 2020"
        }
        
    
  

  
  

  

  

  
      ,"page3": {
          "title": "Resources",
          "content": "Coming Soon .",
          "url": "https://adityassrana.github.io/blog/resources/",
          "relUrl": "/resources/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://adityassrana.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}